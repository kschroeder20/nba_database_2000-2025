{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# NBA Database Scraper (2000-Present)\n",
    "\n",
    "This notebook scrapes Basketball Reference to build a SQLite database of NBA data.\n",
    "\n",
    "**Tables:**\n",
    "- `players` — player biographical info\n",
    "- `teams` — team identifiers and conference/division\n",
    "- `player_season_stats` — per-game and advanced stats by season\n",
    "- `team_season_stats` — team record and ratings by season\n",
    "- `games` — individual game results\n",
    "\n",
    "**Seasons:** 1999-2000 through 2024-25 (referenced as 2000-2025 in Basketball Reference URLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1000001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import sqlite3\n",
    "import time\n",
    "import re\n",
    "\n",
    "DB_PATH = \"nba.db\"\n",
    "BASE_URL = \"https://www.basketball-reference.com\"\n",
    "DELAY = 3\n",
    "\n",
    "# Season range\n",
    "SEASONS = list(range(2000, 2026))\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Config loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. Database Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2000001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tables created\n"
     ]
    }
   ],
   "source": [
    "# Create database tables\n",
    "# Check if they exist first\n",
    "def create_tables():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS players (\n",
    "            player_id TEXT PRIMARY KEY,\n",
    "            full_name TEXT,\n",
    "            birth_date TEXT,\n",
    "            height TEXT,\n",
    "            weight INTEGER,\n",
    "            position TEXT,\n",
    "            shoots TEXT,\n",
    "            draft_year INTEGER,\n",
    "            draft_round INTEGER,\n",
    "            draft_pick INTEGER,\n",
    "            draft_team TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS teams (\n",
    "            team_id TEXT PRIMARY KEY,\n",
    "            team_name TEXT,\n",
    "            conference TEXT,\n",
    "            division TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS player_season_stats (\n",
    "            player_id TEXT,\n",
    "            season INTEGER,\n",
    "            team_id TEXT,\n",
    "            games INTEGER,\n",
    "            games_started INTEGER,\n",
    "            minutes REAL,\n",
    "            points REAL,\n",
    "            rebounds REAL,\n",
    "            assists REAL,\n",
    "            per REAL,\n",
    "            ts_pct REAL,\n",
    "            ws REAL,\n",
    "            bpm REAL,\n",
    "            vorp REAL,\n",
    "            PRIMARY KEY (player_id, season, team_id)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS team_season_stats (\n",
    "            team_id TEXT,\n",
    "            season INTEGER,\n",
    "            wins INTEGER,\n",
    "            losses INTEGER,\n",
    "            win_pct REAL,\n",
    "            pace REAL,\n",
    "            offensive_rating REAL,\n",
    "            defensive_rating REAL,\n",
    "            srs REAL,\n",
    "            PRIMARY KEY (team_id, season)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS games (\n",
    "            game_id TEXT PRIMARY KEY,\n",
    "            season INTEGER,\n",
    "            game_date TEXT,\n",
    "            home_team TEXT,\n",
    "            away_team TEXT,\n",
    "            home_score INTEGER,\n",
    "            away_score INTEGER,\n",
    "            home_win INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"tables created\")\n",
    "\n",
    "\n",
    "create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000001",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3000001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready\n"
     ]
    }
   ],
   "source": [
    "# Use BeautifulSoup to fetch and parse pages\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        time.sleep(DELAY)\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"fetch failed for {url} -- {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Helper functions for parsing tables using BeautifulSoup\n",
    "def find_table(soup, table_id):\n",
    "    # Try normal search first\n",
    "    table = soup.find(\"table\", {\"id\": table_id})\n",
    "    if table is not None:\n",
    "        return table\n",
    "\n",
    "    # Search inside HTML comments\n",
    "    # Comments are not parsed as part of the normal DOM, so we have to look through them manually\n",
    "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        comment_text = str(comment)\n",
    "        if table_id not in comment_text:\n",
    "            continue\n",
    "        comment_soup = BeautifulSoup(comment_text, \"html.parser\")\n",
    "        table = comment_soup.find(\"table\", {\"id\": table_id})\n",
    "        if table is not None:\n",
    "            return table\n",
    "\n",
    "    return None\n",
    "\n",
    "# Converts string -> int or None\n",
    "def safe_int(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    value = str(value).strip().replace(\",\", \"\")\n",
    "    if value == \"\" or value == \"-\":\n",
    "        return None\n",
    "    try:\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Converts string -> float or None\n",
    "def safe_float(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    value = str(value).strip().replace(\",\", \"\")\n",
    "    if value == \"\" or value == \"-\":\n",
    "        return None\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Extracts text content of a table cell\n",
    "def get_cell_text(row, stat_name):\n",
    "    cell = row.find(\"td\", {\"data-stat\": stat_name})\n",
    "    if cell is None:\n",
    "        cell = row.find(\"th\", {\"data-stat\": stat_name})\n",
    "    if cell is None:\n",
    "        return \"\"\n",
    "    return cell.get_text(strip=True)\n",
    "\n",
    "\n",
    "# Extracts links from a table cell\n",
    "def get_cell_link(row, stat_name):\n",
    "    cell = row.find(\"td\", {\"data-stat\": stat_name})\n",
    "    if cell is None:\n",
    "        cell = row.find(\"th\", {\"data-stat\": stat_name})\n",
    "    if cell is None:\n",
    "        return None\n",
    "    link = cell.find(\"a\")\n",
    "    if link is None:\n",
    "        return None\n",
    "    return link.get(\"href\", None)\n",
    "\n",
    "# Prints row to log file\n",
    "# Prevents cell outputs from being too long\n",
    "def log_row(row):\n",
    "    with open(\"scrape_log.txt\", \"a\") as f:\n",
    "        f.write(str(row) + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"Helpers ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. Scrape Teams from Season Pages\n",
    "\n",
    "For each season, load the season summary page and extract the team names,\n",
    "IDs, conferences, and divisions from the standings tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape teams for a given season\n",
    "def scrape_teams_for_season(season):\n",
    "    url = f\"{BASE_URL}/leagues/NBA_{season}.html\"\n",
    "    season_label = f\"{season - 1}-{str(season)[2:]}\"\n",
    "    print(f\"[teams]: {season_label}\")\n",
    "    log_row(f\"scraping teams for season: {season_label}\")\n",
    "\n",
    "    soup = get_soup(url)\n",
    "    if soup is None:\n",
    "        raise RuntimeError(f\"Failed to fetch {url}\")\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    team_ids = []\n",
    "\n",
    "    # Basketball Reference has Eastern and Western conference standings.\n",
    "    # Table IDs vary by year; try multiple possibilities.\n",
    "    conference_tables = [\n",
    "        (\"E\", \"Eastern\"),\n",
    "        (\"W\", \"Western\"),\n",
    "    ]\n",
    "\n",
    "    for conf_code, conf_name in conference_tables:\n",
    "        table = find_table(soup, f\"divs_standings_{conf_code}\")\n",
    "        if table is None:\n",
    "            table = find_table(soup, f\"confs_standings_{conf_code}\")\n",
    "\n",
    "        if table is None:\n",
    "            print(f\"(missing {conf_name} standings) \", end=\"\")\n",
    "            continue\n",
    "\n",
    "        # Division headers are interspersed in the table, \n",
    "        # so track the current division as we go\n",
    "        current_division = \"\"\n",
    "        all_rows = table.find_all(\"tr\")\n",
    "\n",
    "        for row in all_rows:\n",
    "            header_th = row.find(\"th\", {\"colspan\": True})\n",
    "            if header_th:\n",
    "                current_division = header_th.get_text(strip=True)\n",
    "                continue\n",
    "\n",
    "            team_cell = row.find(\"th\", {\"data-stat\": \"team_name\"})\n",
    "            if team_cell is None:\n",
    "                team_cell = row.find(\"td\", {\"data-stat\": \"team_name\"})\n",
    "            if team_cell is None:\n",
    "                continue\n",
    "\n",
    "            link = team_cell.find(\"a\")\n",
    "            if link is None:\n",
    "                continue\n",
    "\n",
    "            href = link.get(\"href\", \"\")\n",
    "            team_name = link.get_text(strip=True)\n",
    "\n",
    "            # Parse team_id from URL like /teams/LAL/2020.html\n",
    "            parts = href.strip(\"/\").split(\"/\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            team_id = parts[1]\n",
    "\n",
    "            team_ids.append(team_id)\n",
    "\n",
    "            cursor.execute(\n",
    "                \"INSERT OR IGNORE INTO teams (team_id, team_name, conference, division) \"\n",
    "                \"VALUES (?, ?, ?, ?)\",\n",
    "                (team_id, team_name, conf_name, current_division),\n",
    "            )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"found {len(team_ids)} teams\")\n",
    "    return team_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. Scrape Team Season Stats\n",
    "\n",
    "For each team-season, load the team page and extract:\n",
    "- Win/loss record from the page meta section\n",
    "- Advanced stats (Pace, ORtg, DRtg, SRS) from the team_misc table\n",
    "- Roster player links (to build the set of player IDs to scrape later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a team's season page for team stats and roster player IDs.\n",
    "def scrape_team_season(team_id, season, all_player_ids):\n",
    "    url = f\"{BASE_URL}/teams/{team_id}/{season}.html\"\n",
    "    print(f\"{team_id} {season} ...\", end=\" \")\n",
    "    log_row(f\"scraping team season: {team_id} {season}\")\n",
    "\n",
    "    soup = get_soup(url)\n",
    "    if soup is None:\n",
    "        raise RuntimeError(f\"Failed to fetch {url}\")\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    wins = None\n",
    "    losses = None\n",
    "\n",
    "    meta_div = soup.find(\"div\", {\"id\": \"meta\"})\n",
    "    if meta_div:\n",
    "        meta_paragraphs = meta_div.find_all(\"p\")\n",
    "        for paragraph in meta_paragraphs:\n",
    "            paragraph_text = paragraph.get_text()\n",
    "            # Look for a line containing the record\n",
    "            has_record_keyword = (\"Record\" in paragraph_text or \"record\" in paragraph_text)\n",
    "            record_match = re.search(r'(\\d+)-(\\d+)', paragraph_text)\n",
    "            if record_match and has_record_keyword:\n",
    "                wins = safe_int(record_match.group(1))\n",
    "                losses = safe_int(record_match.group(2))\n",
    "                break\n",
    "\n",
    "    # Compute win percentage\n",
    "    win_pct = None\n",
    "    if wins is not None and losses is not None:\n",
    "        total_games = wins + losses\n",
    "        if total_games > 0:\n",
    "            win_pct = round(wins / total_games, 3)\n",
    "\n",
    "    # Extract advanced team stats from team_misc table\n",
    "    pace = None\n",
    "    offensive_rating = None\n",
    "    defensive_rating = None\n",
    "    srs = None\n",
    "\n",
    "    misc_table = find_table(soup, \"team_misc\")\n",
    "    if misc_table:\n",
    "        # The table has thead (column headers) and tbody (data rows).\n",
    "        # We only want tbody rows -- otherwise we match the header text\n",
    "        # like \"Pace\" instead of the actual value like \"89.7\".\n",
    "        tbody = misc_table.find(\"tbody\")\n",
    "        if tbody:\n",
    "            data_rows = tbody.find_all(\"tr\")\n",
    "        else:\n",
    "            data_rows = misc_table.find_all(\"tr\")\n",
    "\n",
    "        for row in data_rows:\n",
    "            pace_text = get_cell_text(row, \"pace\")\n",
    "            if pace_text == \"\":\n",
    "                continue\n",
    "            pace_val = safe_float(pace_text)\n",
    "            if pace_val is None:\n",
    "                # this is a non-numeric row (like \"Lg Rank\"), skip it\n",
    "                continue\n",
    "            pace = pace_val\n",
    "            srs = safe_float(get_cell_text(row, \"srs\"))\n",
    "            offensive_rating = safe_float(get_cell_text(row, \"off_rtg\"))\n",
    "            defensive_rating = safe_float(get_cell_text(row, \"def_rtg\"))\n",
    "            break\n",
    "\n",
    "    cursor.execute(\n",
    "        \"INSERT OR REPLACE INTO team_season_stats \"\n",
    "        \"(team_id, season, wins, losses, win_pct, pace, \"\n",
    "        \"offensive_rating, defensive_rating, srs) \"\n",
    "        \"VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "        (team_id, season, wins, losses, win_pct, pace,\n",
    "         offensive_rating, defensive_rating, srs),\n",
    "    )\n",
    "\n",
    "    # Extract roster player IDs\n",
    "    roster_table = find_table(soup, \"roster\")\n",
    "    roster_count = 0\n",
    "    if roster_table:\n",
    "        roster_rows = roster_table.find_all(\"tr\")\n",
    "        for row in roster_rows:\n",
    "            links = row.find_all(\"a\")\n",
    "            for link in links:\n",
    "                href = link.get(\"href\", \"\")\n",
    "                if \"/players/\" in href and href.endswith(\".html\"):\n",
    "                    parts = href.strip(\"/\").split(\"/\")\n",
    "                    if len(parts) >= 3:\n",
    "                        player_id = parts[2].replace(\".html\", \"\")\n",
    "                        all_player_ids.add(player_id)\n",
    "                        roster_count += 1\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    log_row(f\"team season: {team_id} {season} -- wins: {wins}, losses: {losses}, win%: {win_pct}, pace: {pace}, off_rtg: {offensive_rating}, def_rtg: {defensive_rating}, srs: {srs}, num_players: {roster_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. Scrape Player Pages (Bio + Season Stats)\n",
    "\n",
    "For each player, load their page once and extract:\n",
    "- Biographical info (name, height, weight, position, draft) from the meta section\n",
    "- Per-game stats from the `per_game` table\n",
    "- Advanced stats (PER, TS%, WS, BPM, VORP) from the `advanced` table\n",
    "\n",
    "The two stat tables are merged by (season, team_id) key before inserting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a player's Basketball Reference page.\n",
    "def scrape_player_page(player_id):\n",
    "    # Player pages are organized by the first letter of the player_id\n",
    "    first_letter = player_id[0]\n",
    "    url = f\"{BASE_URL}/players/{first_letter}/{player_id}.html\"\n",
    "\n",
    "    soup = get_soup(url)\n",
    "    if soup is None:\n",
    "        raise RuntimeError(f\"Failed to fetch {url}\")\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    ## Player bio and draft info\n",
    "\n",
    "    full_name = \"\"\n",
    "    birth_date = None\n",
    "    height = None\n",
    "    weight = None\n",
    "    position = None\n",
    "    shoots = None\n",
    "    draft_year = None\n",
    "    draft_round = None\n",
    "    draft_pick = None\n",
    "    draft_team = None\n",
    "\n",
    "    # Name\n",
    "    heading = soup.find(\"h1\")\n",
    "    if heading:\n",
    "        name_span = heading.find(\"span\")\n",
    "        if name_span:\n",
    "            full_name = name_span.get_text(strip=True)\n",
    "        else:\n",
    "            full_name = heading.get_text(strip=True)\n",
    "\n",
    "    # Meta section fields\n",
    "    meta_div = soup.find(\"div\", {\"id\": \"meta\"})\n",
    "    if meta_div:\n",
    "        paragraphs = meta_div.find_all(\"p\")\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            text = paragraph.get_text(strip=True)\n",
    "\n",
    "            # Position\n",
    "            if \"Position:\" in text:\n",
    "                pos_match = re.search(\n",
    "                    r'Position:\\s*(.+?)(?:\\s*\\u25aa|\\s*Shoots:|$)', text\n",
    "                )\n",
    "                if pos_match:\n",
    "                    position = pos_match.group(1).strip()\n",
    "\n",
    "            # Shooting hand\n",
    "            if \"Shoots:\" in text:\n",
    "                shoots_match = re.search(r'Shoots:\\s*(\\w+)', text)\n",
    "                if shoots_match:\n",
    "                    shoots = shoots_match.group(1).strip()\n",
    "\n",
    "            # Height / weight\n",
    "            height_match = re.search(r'(\\d+-\\d+)', text)\n",
    "            weight_match = re.search(r'(\\d+)lb', text)\n",
    "            if height_match and (\"lb\" in text or \"cm\" in text):\n",
    "                height = height_match.group(1)\n",
    "            if weight_match:\n",
    "                weight = safe_int(weight_match.group(1))\n",
    "\n",
    "            # Draft info\n",
    "            if \"Draft:\" in text:\n",
    "                draft_team_match = re.search(r'Draft:\\s*(.+?),', text)\n",
    "                if draft_team_match:\n",
    "                    draft_team = draft_team_match.group(1).strip()\n",
    "\n",
    "                round_match = re.search(r'(\\d+)\\w*\\s*round', text)\n",
    "                if round_match:\n",
    "                    draft_round = safe_int(round_match.group(1))\n",
    "\n",
    "                pick_match = re.search(r'(\\d+)\\w*\\s*pick', text)\n",
    "                if pick_match:\n",
    "                    draft_pick = safe_int(pick_match.group(1))\n",
    "\n",
    "                year_match = re.search(r'(\\d{4})\\s*NBA\\s*Draft', text)\n",
    "                if year_match:\n",
    "                    draft_year = safe_int(year_match.group(1))\n",
    "\n",
    "        # Birth date\n",
    "        birth_span = meta_div.find(\"span\", {\"id\": \"necro-birth\"})\n",
    "        if birth_span:\n",
    "            birth_date = birth_span.get(\"data-birth\", None)\n",
    "            if birth_date is None:\n",
    "                birth_date = birth_span.get_text(strip=True)\n",
    "\n",
    "    cursor.execute(\n",
    "        \"INSERT OR REPLACE INTO players \"\n",
    "        \"(player_id, full_name, birth_date, height, weight, position, shoots, \"\n",
    "        \"draft_year, draft_round, draft_pick, draft_team) \"\n",
    "        \"VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "        (player_id, full_name, birth_date, height, weight, position, shoots,\n",
    "         draft_year, draft_round, draft_pick, draft_team),\n",
    "    )\n",
    "\n",
    "    ## Player season stats\n",
    "    \n",
    "    ### Per-game season stats\n",
    "    per_game_data = {}  # (season_year, team_id): {games, games_started, minutes, points, rebounds, assists}\n",
    "\n",
    "    # Basketball Reference uses table id \"per_game_stats\" (not \"per_game\")\n",
    "    per_game_table = find_table(soup, \"per_game_stats\")\n",
    "    if per_game_table:\n",
    "        tbody = per_game_table.find(\"tbody\")\n",
    "        rows = tbody.find_all(\"tr\") if tbody else per_game_table.find_all(\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            # Skip sub-header rows\n",
    "            row_classes = row.get(\"class\", [])\n",
    "            if \"thead\" in row_classes or \"partial_table\" in row_classes:\n",
    "                continue\n",
    "\n",
    "            # Parse season text like \"2019-20\" -> season_year = 2020\n",
    "            # Basketball Reference uses data-stat=\"year_id\" for the season column\n",
    "            season_text = get_cell_text(row, \"year_id\")\n",
    "            if season_text == \"\":\n",
    "                continue\n",
    "            season_match = re.match(r'(\\d{4})-(\\d{2})', season_text)\n",
    "            if season_match is None:\n",
    "                continue\n",
    "            season_year = int(season_match.group(1)) + 1\n",
    "\n",
    "            if season_year < 2000 or season_year > 2025:\n",
    "                continue\n",
    "\n",
    "            # Get team -- data-stat is \"team_name_abbr\" on player pages\n",
    "            team_text = get_cell_text(row, \"team_name_abbr\")\n",
    "            if team_text == \"TOT\":\n",
    "                continue\n",
    "\n",
    "            team_link = get_cell_link(row, \"team_name_abbr\")\n",
    "            if team_link:\n",
    "                link_parts = team_link.strip(\"/\").split(\"/\")\n",
    "                team_id_val = link_parts[1] if len(link_parts) >= 2 else team_text\n",
    "            else:\n",
    "                team_id_val = team_text\n",
    "\n",
    "            if team_id_val == \"\":\n",
    "                continue\n",
    "\n",
    "            # data-stat names: \"games\", \"games_started\" (not \"g\", \"gs\")\n",
    "            games = safe_int(get_cell_text(row, \"games\"))\n",
    "            games_started = safe_int(get_cell_text(row, \"games_started\"))\n",
    "            minutes = safe_float(get_cell_text(row, \"mp_per_g\"))\n",
    "            points = safe_float(get_cell_text(row, \"pts_per_g\"))\n",
    "            rebounds = safe_float(get_cell_text(row, \"trb_per_g\"))\n",
    "            assists = safe_float(get_cell_text(row, \"ast_per_g\"))\n",
    "\n",
    "            key = (season_year, team_id_val)\n",
    "            per_game_data[key] = {\n",
    "                \"games\": games,\n",
    "                \"games_started\": games_started,\n",
    "                \"minutes\": minutes,\n",
    "                \"points\": points,\n",
    "                \"rebounds\": rebounds,\n",
    "                \"assists\": assists,\n",
    "            }\n",
    "\n",
    "    ### Advanced season stats\n",
    "    advanced_data = {}  # (season_year, team_id): {per, ts_pct, ws, bpm, vorp}\n",
    "\n",
    "    advanced_table = find_table(soup, \"advanced\")\n",
    "    if advanced_table:\n",
    "        tbody = advanced_table.find(\"tbody\")\n",
    "        rows = tbody.find_all(\"tr\") if tbody else advanced_table.find_all(\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            row_classes = row.get(\"class\", [])\n",
    "            if \"thead\" in row_classes or \"partial_table\" in row_classes:\n",
    "                continue\n",
    "\n",
    "            season_text = get_cell_text(row, \"year_id\")\n",
    "            if season_text == \"\":\n",
    "                continue\n",
    "            season_match = re.match(r'(\\d{4})-(\\d{2})', season_text)\n",
    "            if season_match is None:\n",
    "                continue\n",
    "            season_year = int(season_match.group(1)) + 1\n",
    "\n",
    "            if season_year < 2000 or season_year > 2025:\n",
    "                continue\n",
    "\n",
    "            team_text = get_cell_text(row, \"team_name_abbr\")\n",
    "            if team_text == \"TOT\":\n",
    "                continue\n",
    "\n",
    "            team_link = get_cell_link(row, \"team_name_abbr\")\n",
    "            if team_link:\n",
    "                link_parts = team_link.strip(\"/\").split(\"/\")\n",
    "                team_id_val = link_parts[1] if len(link_parts) >= 2 else team_text\n",
    "            else:\n",
    "                team_id_val = team_text\n",
    "\n",
    "            per_val = safe_float(get_cell_text(row, \"per\"))\n",
    "            ts_pct = safe_float(get_cell_text(row, \"ts_pct\"))\n",
    "            ws = safe_float(get_cell_text(row, \"ws\"))\n",
    "            bpm = safe_float(get_cell_text(row, \"bpm\"))\n",
    "            vorp = safe_float(get_cell_text(row, \"vorp\"))\n",
    "\n",
    "            key = (season_year, team_id_val)\n",
    "            advanced_data[key] = {\n",
    "                \"per\": per_val,\n",
    "                \"ts_pct\": ts_pct,\n",
    "                \"ws\": ws,\n",
    "                \"bpm\": bpm,\n",
    "                \"vorp\": vorp,\n",
    "            }\n",
    "\n",
    "    ### Merge per-game and advanced, then insert\n",
    "    all_keys = set(per_game_data.keys()) | set(advanced_data.keys())\n",
    "\n",
    "    for key in all_keys:\n",
    "        season_year, team_id_val = key\n",
    "\n",
    "        pg = per_game_data.get(key, {})\n",
    "        adv = advanced_data.get(key, {})\n",
    "\n",
    "        cursor.execute(\n",
    "            \"INSERT OR REPLACE INTO player_season_stats \"\n",
    "            \"(player_id, season, team_id, games, games_started, minutes, \"\n",
    "            \"points, rebounds, assists, per, ts_pct, ws, bpm, vorp) \"\n",
    "            \"VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (\n",
    "                player_id,\n",
    "                season_year,\n",
    "                team_id_val,\n",
    "                pg.get(\"games\"),\n",
    "                pg.get(\"games_started\"),\n",
    "                pg.get(\"minutes\"),\n",
    "                pg.get(\"points\"),\n",
    "                pg.get(\"rebounds\"),\n",
    "                pg.get(\"assists\"),\n",
    "                adv.get(\"per\"),\n",
    "                adv.get(\"ts_pct\"),\n",
    "                adv.get(\"ws\"),\n",
    "                adv.get(\"bpm\"),\n",
    "                adv.get(\"vorp\"),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    log_row(f\"player: {full_name} -- {len(all_keys)} season entries inserted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## 7. Scrape Games\n",
    "\n",
    "For each season, load the schedule page and follow the monthly sub-links.\n",
    "Each month page contains a schedule table with game results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all games for a season, including date, teams, scores, and winner.\n",
    "def scrape_games_for_season(season):\n",
    "    # Games are organized by season on a main schedule page, \n",
    "    # which links to monthly sub-pages for some seasons.\n",
    "    url = f\"{BASE_URL}/leagues/NBA_{season}_games.html\"\n",
    "    season_label = f\"{season - 1}-{str(season)[2:]}\"\n",
    "    print(f\"[games] {season_label} ...\", end=\" \")\n",
    "    log_row(f\"scraping games for season: {season_label}\")\n",
    "\n",
    "    soup = get_soup(url)\n",
    "    if soup is None:\n",
    "        raise RuntimeError(f\"Failed to fetch {url}\")\n",
    "\n",
    "    # Collect monthly schedule page links\n",
    "    month_links = []\n",
    "    filter_div = soup.find(\"div\", {\"class\": \"filter\"})\n",
    "    if filter_div:\n",
    "        links = filter_div.find_all(\"a\")\n",
    "        for link in links:\n",
    "            href = link.get(\"href\", \"\")\n",
    "            if \"games\" in href:\n",
    "                full_url = BASE_URL + href\n",
    "                month_links.append(full_url)\n",
    "\n",
    "    # If no month links found, the current page may have the schedule directly\n",
    "    if len(month_links) == 0:\n",
    "        month_links = [url]\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    games_inserted = 0\n",
    "\n",
    "    for month_url in month_links:\n",
    "        # Avoid re-fetching the page we already have\n",
    "        if month_url == url:\n",
    "            month_soup = soup\n",
    "        else:\n",
    "            month_soup = get_soup(month_url)\n",
    "\n",
    "        if month_soup is None:\n",
    "            raise RuntimeError(f\"Failed to fetch month page {month_url}\")\n",
    "\n",
    "        schedule_table = find_table(month_soup, \"schedule\")\n",
    "        if schedule_table is None:\n",
    "            continue\n",
    "\n",
    "        tbody = schedule_table.find(\"tbody\")\n",
    "        rows = tbody.find_all(\"tr\") if tbody else schedule_table.find_all(\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            # Skip header rows\n",
    "            row_classes = row.get(\"class\", [])\n",
    "            if \"thead\" in row_classes:\n",
    "                continue\n",
    "\n",
    "            # Box score link gives us the game_id\n",
    "            box_link = get_cell_link(row, \"box_score_text\")\n",
    "            if box_link is None:\n",
    "                continue\n",
    "\n",
    "            # Parse game_id from box link (i.e. /boxscores/202001010LAL.html)\n",
    "            game_id_with_ext = box_link.split(\"/\")[-1]\n",
    "            game_id = game_id_with_ext.replace(\".html\", \"\")\n",
    "\n",
    "            # Date\n",
    "            date_cell = row.find(\"th\", {\"data-stat\": \"date_game\"})\n",
    "            if date_cell is None:\n",
    "                continue\n",
    "            game_date = date_cell.get(\"csk\", date_cell.get_text(strip=True))\n",
    "\n",
    "            # Away team\n",
    "            away_link = get_cell_link(row, \"visitor_team_name\")\n",
    "            away_team_id = \"\"\n",
    "            if away_link:\n",
    "                parts = away_link.strip(\"/\").split(\"/\")\n",
    "                if len(parts) >= 2:\n",
    "                    away_team_id = parts[1]\n",
    "\n",
    "            # Home team\n",
    "            home_link = get_cell_link(row, \"home_team_name\")\n",
    "            home_team_id = \"\"\n",
    "            if home_link:\n",
    "                parts = home_link.strip(\"/\").split(\"/\")\n",
    "                if len(parts) >= 2:\n",
    "                    home_team_id = parts[1]\n",
    "\n",
    "            # Scores\n",
    "            away_score = safe_int(get_cell_text(row, \"visitor_pts\"))\n",
    "            home_score = safe_int(get_cell_text(row, \"home_pts\"))\n",
    "\n",
    "            # Skip rows with missing scores (e.g., future games)\n",
    "            if home_score is None or away_score is None:\n",
    "                continue\n",
    "\n",
    "            home_win = 1 if home_score > away_score else 0\n",
    "\n",
    "            cursor.execute(\n",
    "                \"INSERT OR REPLACE INTO games \"\n",
    "                \"(game_id, season, game_date, home_team, away_team, \"\n",
    "                \"home_score, away_score, home_win) \"\n",
    "                \"VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "                (game_id, season, game_date, home_team_id, away_team_id,\n",
    "                 home_score, away_score, home_win),\n",
    "            )\n",
    "            games_inserted += 1\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"{games_inserted} games inserted\")\n",
    "    log_row(f\"total games inserted for season {season_label}: {games_inserted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000001",
   "metadata": {},
   "source": [
    "## 8. Main Orchestration\n",
    "\n",
    "Run the full scraping pipeline:\n",
    "1. Create tables\n",
    "2. For each season: scrape teams, then scrape each team's season page (collecting player IDs)\n",
    "3. Scrape all discovered player pages\n",
    "4. Scrape game results for each season\n",
    "\n",
    "Each step uses `INSERT OR REPLACE`, so the script can be safely re-run if interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8000001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tables created\n",
      "[teams]: 1999-00\n",
      "found 29 teams\n",
      "[teams]: 2000-01\n",
      "found 29 teams\n",
      "[teams]: 2001-02\n",
      "found 29 teams\n",
      "[teams]: 2002-03\n",
      "found 29 teams\n",
      "[teams]: 2003-04\n",
      "found 29 teams\n",
      "[teams]: 2004-05\n",
      "found 30 teams\n",
      "[teams]: 2005-06\n",
      "found 30 teams\n",
      "[teams]: 2006-07\n",
      "found 30 teams\n",
      "[teams]: 2007-08\n",
      "found 30 teams\n",
      "[teams]: 2008-09\n",
      "found 30 teams\n",
      "[teams]: 2009-10\n",
      "found 30 teams\n",
      "[teams]: 2010-11\n",
      "found 30 teams\n",
      "[teams]: 2011-12\n",
      "found 30 teams\n",
      "[teams]: 2012-13\n",
      "found 30 teams\n",
      "[teams]: 2013-14\n",
      "found 30 teams\n",
      "[teams]: 2014-15\n",
      "found 30 teams\n",
      "[teams]: 2015-16\n",
      "found 30 teams\n",
      "[teams]: 2016-17\n",
      "found 30 teams\n",
      "[teams]: 2017-18\n",
      "found 30 teams\n",
      "[teams]: 2018-19\n",
      "found 30 teams\n",
      "[teams]: 2019-20\n",
      "found 30 teams\n",
      "[teams]: 2020-21\n",
      "found 30 teams\n",
      "[teams]: 2021-22\n",
      "found 30 teams\n",
      "[teams]: 2022-23\n",
      "found 30 teams\n",
      "[teams]: 2023-24\n",
      "found 30 teams\n",
      "[teams]: 2024-25\n",
      "found 30 teams\n",
      "\n",
      "Init scraping team season pages for all teams and seasons\n",
      "MIA 2000 ... NYK 2000 ... PHI 2000 ... ORL 2000 ... BOS 2000 ... NJN 2000 ... WAS 2000 ... IND 2000 ... CHH 2000 ... TOR 2000 ... DET 2000 ... MIL 2000 ... CLE 2000 ... ATL 2000 ... CHI 2000 ... UTA 2000 ... SAS 2000 ... MIN 2000 ... DAL 2000 ... DEN 2000 ... HOU 2000 ... VAN 2000 ... LAL 2000 ... POR 2000 ... PHO 2000 ... SEA 2000 ... SAC 2000 ... GSW 2000 ... LAC 2000 ... PHI 2001 ... MIA 2001 ... NYK 2001 ... ORL 2001 ... BOS 2001 ... NJN 2001 ... WAS 2001 ... MIL 2001 ... TOR 2001 ... CHH 2001 ... IND 2001 ... DET 2001 ... CLE 2001 ... ATL 2001 ... CHI 2001 ... SAS 2001 ... UTA 2001 ... DAL 2001 ... MIN 2001 ... HOU 2001 ... DEN 2001 ... VAN 2001 ... LAL 2001 ... SAC 2001 ... PHO 2001 ... POR 2001 ... SEA 2001 ... LAC 2001 ... GSW 2001 ... NJN 2002 ... BOS 2002 ... ORL 2002 ... PHI 2002 ... WAS 2002 ... MIA 2002 ... NYK 2002 ... DET 2002 ... CHH 2002 ... TOR 2002 ... IND 2002 ... MIL 2002 ... ATL 2002 ... CLE 2002 ... CHI 2002 ... SAS 2002 ... DAL 2002 ... MIN 2002 ... UTA 2002 ... HOU 2002 ... DEN 2002 ... MEM 2002 ... SAC 2002 ... LAL 2002 ... POR 2002 ... SEA 2002 ... LAC 2002 ... PHO 2002 ... GSW 2002 ... NJN 2003 ... PHI 2003 ... BOS 2003 ... ORL 2003 ... WAS 2003 ... NYK 2003 ... MIA 2003 ... DET 2003 ... IND 2003 ... NOH 2003 ... MIL 2003 ... ATL 2003 ... CHI 2003 ... TOR 2003 ... CLE 2003 ... SAS 2003 ... DAL 2003 ... MIN 2003 ... UTA 2003 ... HOU 2003 ... MEM 2003 ... DEN 2003 ... SAC 2003 ... LAL 2003 ... POR 2003 ... PHO 2003 ... SEA 2003 ... GSW 2003 ... LAC 2003 ... NJN 2004 ... MIA 2004 ... NYK 2004 ... BOS 2004 ... PHI 2004 ... WAS 2004 ... ORL 2004 ... IND 2004 ... DET 2004 ... NOH 2004 ... MIL 2004 ... CLE 2004 ... TOR 2004 ... ATL 2004 ... CHI 2004 ... MIN 2004 ... SAS 2004 ... DAL 2004 ... MEM 2004 ... HOU 2004 ... DEN 2004 ... UTA 2004 ... LAL 2004 ... SAC 2004 ... POR 2004 ... GSW 2004 ... SEA 2004 ... PHO 2004 ... LAC 2004 ... BOS 2005 ... PHI 2005 ... NJN 2005 ... TOR 2005 ... NYK 2005 ... DET 2005 ... CHI 2005 ... IND 2005 ... CLE 2005 ... MIL 2005 ... MIA 2005 ... WAS 2005 ... ORL 2005 ... CHA 2005 ... ATL 2005 ... SEA 2005 ... DEN 2005 ... MIN 2005 ... POR 2005 ... UTA 2005 ... PHO 2005 ... SAC 2005 ... LAC 2005 ... LAL 2005 ... GSW 2005 ... SAS 2005 ... DAL 2005 ... HOU 2005 ... MEM 2005 ... NOH 2005 ... NJN 2006 ... PHI 2006 ... BOS 2006 ... TOR 2006 ... NYK 2006 ... DET 2006 ... CLE 2006 ... IND 2006 ... CHI 2006 ... MIL 2006 ... MIA 2006 ... WAS 2006 ... ORL 2006 ... CHA 2006 ... ATL 2006 ... DEN 2006 ... UTA 2006 ... SEA 2006 ... MIN 2006 ... POR 2006 ... PHO 2006 ... LAC 2006 ... LAL 2006 ... SAC 2006 ... GSW 2006 ... SAS 2006 ... DAL 2006 ... MEM 2006 ... NOK 2006 ... HOU 2006 ... TOR 2007 ... NJN 2007 ... PHI 2007 ... NYK 2007 ... BOS 2007 ... DET 2007 ... CLE 2007 ... CHI 2007 ... IND 2007 ... MIL 2007 ... MIA 2007 ... WAS 2007 ... ORL 2007 ... CHA 2007 ... ATL 2007 ... UTA 2007 ... DEN 2007 ... POR 2007 ... MIN 2007 ... SEA 2007 ... PHO 2007 ... LAL 2007 ... GSW 2007 ... LAC 2007 ... SAC 2007 ... DAL 2007 ... SAS 2007 ... HOU 2007 ... NOK 2007 ... MEM 2007 ... BOS 2008 ... TOR 2008 ... PHI 2008 ... NJN 2008 ... NYK 2008 ... DET 2008 ... CLE 2008 ... IND 2008 ... CHI 2008 ... MIL 2008 ... ORL 2008 ... WAS 2008 ... ATL 2008 ... CHA 2008 ... MIA 2008 ... UTA 2008 ... DEN 2008 ... POR 2008 ... MIN 2008 ... SEA 2008 ... LAL 2008 ... PHO 2008 ... GSW 2008 ... SAC 2008 ... LAC 2008 ... NOH 2008 ... SAS 2008 ... HOU 2008 ... DAL 2008 ... MEM 2008 ... BOS 2009 ... PHI 2009 ... NJN 2009 ... TOR 2009 ... NYK 2009 ... CLE 2009 ... CHI 2009 ... DET 2009 ... IND 2009 ... MIL 2009 ... ORL 2009 ... ATL 2009 ... MIA 2009 ... CHA 2009 ... WAS 2009 ... DEN 2009 ... POR 2009 ... UTA 2009 ... MIN 2009 ... OKC 2009 ... LAL 2009 ... PHO 2009 ... GSW 2009 ... LAC 2009 ... SAC 2009 ... SAS 2009 ... HOU 2009 ... DAL 2009 ... NOH 2009 ... MEM 2009 ... BOS 2010 ... TOR 2010 ... NYK 2010 ... PHI 2010 ... fetch failed for https://www.basketball-reference.com/teams/PHI/2010.html -- HTTPSConnectionPool(host='www.basketball-reference.com', port=443): Read timed out. (read timeout=None)\n",
      "NJN 2010 ... CLE 2010 ... MIL 2010 ... CHI 2010 ... IND 2010 ... DET 2010 ... ORL 2010 ... ATL 2010 ... MIA 2010 ... CHA 2010 ... WAS 2010 ... DEN 2010 ... UTA 2010 ... POR 2010 ... OKC 2010 ... MIN 2010 ... LAL 2010 ... PHO 2010 ... LAC 2010 ... GSW 2010 ... SAC 2010 ... DAL 2010 ... SAS 2010 ... HOU 2010 ... MEM 2010 ... NOH 2010 ... BOS 2011 ... NYK 2011 ... PHI 2011 ... NJN 2011 ... TOR 2011 ... CHI 2011 ... IND 2011 ... MIL 2011 ... DET 2011 ... CLE 2011 ... MIA 2011 ... ORL 2011 ... ATL 2011 ... CHA 2011 ... WAS 2011 ... OKC 2011 ... DEN 2011 ... POR 2011 ... UTA 2011 ... MIN 2011 ... LAL 2011 ... PHO 2011 ... GSW 2011 ... LAC 2011 ... SAC 2011 ... SAS 2011 ... DAL 2011 ... NOH 2011 ... MEM 2011 ... HOU 2011 ... BOS 2012 ... NYK 2012 ... PHI 2012 ... TOR 2012 ... NJN 2012 ... CHI 2012 ... IND 2012 ... MIL 2012 ... DET 2012 ... CLE 2012 ... MIA 2012 ... ATL 2012 ... ORL 2012 ... WAS 2012 ... CHA 2012 ... OKC 2012 ... DEN 2012 ... UTA 2012 ... POR 2012 ... MIN 2012 ... LAL 2012 ... LAC 2012 ... PHO 2012 ... GSW 2012 ... SAC 2012 ... SAS 2012 ... MEM 2012 ... DAL 2012 ... HOU 2012 ... NOH 2012 ... NYK 2013 ... BRK 2013 ... BOS 2013 ... PHI 2013 ... TOR 2013 ... IND 2013 ... CHI 2013 ... MIL 2013 ... DET 2013 ... CLE 2013 ... MIA 2013 ... ATL 2013 ... WAS 2013 ... CHA 2013 ... ORL 2013 ... OKC 2013 ... DEN 2013 ... UTA 2013 ... POR 2013 ... MIN 2013 ... LAC 2013 ... GSW 2013 ... LAL 2013 ... SAC 2013 ... PHO 2013 ... SAS 2013 ... MEM 2013 ... HOU 2013 ... DAL 2013 ... NOH 2013 ... TOR 2014 ... BRK 2014 ... NYK 2014 ... BOS 2014 ... PHI 2014 ... IND 2014 ... CHI 2014 ... CLE 2014 ... DET 2014 ... MIL 2014 ... MIA 2014 ... WAS 2014 ... CHA 2014 ... ATL 2014 ... ORL 2014 ... OKC 2014 ... POR 2014 ... MIN 2014 ... DEN 2014 ... UTA 2014 ... LAC 2014 ... GSW 2014 ... PHO 2014 ... SAC 2014 ... LAL 2014 ... SAS 2014 ... HOU 2014 ... MEM 2014 ... DAL 2014 ... NOP 2014 ... TOR 2015 ... BOS 2015 ... BRK 2015 ... PHI 2015 ... NYK 2015 ... CLE 2015 ... CHI 2015 ... MIL 2015 ... IND 2015 ... DET 2015 ... ATL 2015 ... WAS 2015 ... MIA 2015 ... CHO 2015 ... ORL 2015 ... POR 2015 ... OKC 2015 ... UTA 2015 ... DEN 2015 ... MIN 2015 ... GSW 2015 ... LAC 2015 ... PHO 2015 ... SAC 2015 ... LAL 2015 ... HOU 2015 ... MEM 2015 ... SAS 2015 ... DAL 2015 ... NOP 2015 ... TOR 2016 ... BOS 2016 ... NYK 2016 ... BRK 2016 ... PHI 2016 ... CLE 2016 ... IND 2016 ... DET 2016 ... CHI 2016 ... MIL 2016 ... MIA 2016 ... ATL 2016 ... CHO 2016 ... WAS 2016 ... ORL 2016 ... OKC 2016 ... POR 2016 ... UTA 2016 ... DEN 2016 ... MIN 2016 ... GSW 2016 ... LAC 2016 ... SAC 2016 ... PHO 2016 ... LAL 2016 ... SAS 2016 ... DAL 2016 ... MEM 2016 ... HOU 2016 ... NOP 2016 ... BOS 2017 ... TOR 2017 ... NYK 2017 ... PHI 2017 ... BRK 2017 ... CLE 2017 ... MIL 2017 ... IND 2017 ... CHI 2017 ... DET 2017 ... WAS 2017 ... ATL 2017 ... MIA 2017 ... CHO 2017 ... ORL 2017 ... UTA 2017 ... OKC 2017 ... POR 2017 ... DEN 2017 ... MIN 2017 ... GSW 2017 ... LAC 2017 ... SAC 2017 ... LAL 2017 ... PHO 2017 ... SAS 2017 ... HOU 2017 ... MEM 2017 ... NOP 2017 ... DAL 2017 ... TOR 2018 ... BOS 2018 ... PHI 2018 ... NYK 2018 ... BRK 2018 ... CLE 2018 ... IND 2018 ... MIL 2018 ... DET 2018 ... CHI 2018 ... MIA 2018 ... WAS 2018 ... CHO 2018 ... ORL 2018 ... ATL 2018 ... POR 2018 ... OKC 2018 ... UTA 2018 ... MIN 2018 ... DEN 2018 ... GSW 2018 ... LAC 2018 ... LAL 2018 ... SAC 2018 ... PHO 2018 ... HOU 2018 ... NOP 2018 ... SAS 2018 ... DAL 2018 ... MEM 2018 ... TOR 2019 ... PHI 2019 ... BOS 2019 ... BRK 2019 ... NYK 2019 ... MIL 2019 ... IND 2019 ... DET 2019 ... CHI 2019 ... CLE 2019 ... ORL 2019 ... CHO 2019 ... MIA 2019 ... WAS 2019 ... ATL 2019 ... DEN 2019 ... POR 2019 ... UTA 2019 ... OKC 2019 ... MIN 2019 ... GSW 2019 ... LAC 2019 ... SAC 2019 ... LAL 2019 ... PHO 2019 ... HOU 2019 ... SAS 2019 ... MEM 2019 ... NOP 2019 ... DAL 2019 ... TOR 2020 ... BOS 2020 ... PHI 2020 ... BRK 2020 ... NYK 2020 ... MIL 2020 ... IND 2020 ... CHI 2020 ... DET 2020 ... CLE 2020 ... MIA 2020 ... ORL 2020 ... CHO 2020 ... WAS 2020 ... ATL 2020 ... DEN 2020 ... OKC 2020 ... UTA 2020 ... POR 2020 ... MIN 2020 ... LAL 2020 ... LAC 2020 ... PHO 2020 ... SAC 2020 ... GSW 2020 ... HOU 2020 ... DAL 2020 ... MEM 2020 ... SAS 2020 ... NOP 2020 ... PHI 2021 ... BRK 2021 ... NYK 2021 ... BOS 2021 ... TOR 2021 ... MIL 2021 ... IND 2021 ... CHI 2021 ... CLE 2021 ... DET 2021 ... ATL 2021 ... MIA 2021 ... WAS 2021 ... CHO 2021 ... ORL 2021 ... UTA 2021 ... DEN 2021 ... POR 2021 ... MIN 2021 ... OKC 2021 ... PHO 2021 ... LAC 2021 ... LAL 2021 ... GSW 2021 ... SAC 2021 ... DAL 2021 ... MEM 2021 ... SAS 2021 ... NOP 2021 ... HOU 2021 ... BOS 2022 ... PHI 2022 ... TOR 2022 ... BRK 2022 ... NYK 2022 ... MIL 2022 ... CHI 2022 ... CLE 2022 ... IND 2022 ... DET 2022 ... MIA 2022 ... ATL 2022 ... CHO 2022 ... WAS 2022 ... ORL 2022 ... UTA 2022 ... DEN 2022 ... MIN 2022 ... POR 2022 ... OKC 2022 ... PHO 2022 ... GSW 2022 ... LAC 2022 ... LAL 2022 ... SAC 2022 ... MEM 2022 ... DAL 2022 ... NOP 2022 ... SAS 2022 ... HOU 2022 ... BOS 2023 ... PHI 2023 ... NYK 2023 ... BRK 2023 ... TOR 2023 ... MIL 2023 ... CLE 2023 ... CHI 2023 ... IND 2023 ... DET 2023 ... MIA 2023 ... ATL 2023 ... WAS 2023 ... ORL 2023 ... CHO 2023 ... DEN 2023 ... MIN 2023 ... OKC 2023 ... UTA 2023 ... POR 2023 ... SAC 2023 ... PHO 2023 ... LAC 2023 ... GSW 2023 ... LAL 2023 ... MEM 2023 ... NOP 2023 ... DAL 2023 ... HOU 2023 ... SAS 2023 ... BOS 2024 ... NYK 2024 ... PHI 2024 ... BRK 2024 ... TOR 2024 ... MIL 2024 ... CLE 2024 ... IND 2024 ... CHI 2024 ... DET 2024 ... ORL 2024 ... MIA 2024 ... ATL 2024 ... CHO 2024 ... WAS 2024 ... OKC 2024 ... DEN 2024 ... MIN 2024 ... UTA 2024 ... POR 2024 ... LAC 2024 ... PHO 2024 ... LAL 2024 ... SAC 2024 ... GSW 2024 ... DAL 2024 ... NOP 2024 ... HOU 2024 ... MEM 2024 ... SAS 2024 ... BOS 2025 ... NYK 2025 ... TOR 2025 ... BRK 2025 ... PHI 2025 ... CLE 2025 ... IND 2025 ... MIL 2025 ... DET 2025 ... CHI 2025 ... ORL 2025 ... ATL 2025 ... MIA 2025 ... CHO 2025 ... WAS 2025 ... OKC 2025 ... DEN 2025 ... MIN 2025 ... POR 2025 ... UTA 2025 ... LAL 2025 ... LAC 2025 ... GSW 2025 ... SAC 2025 ... PHO 2025 ... HOU 2025 ... MEM 2025 ... DAL 2025 ... SAS 2025 ... NOP 2025 ... PHI 2010 ... \n",
      "total unique player IDs collected from team rosters: 2551\n",
      "\n",
      "Init scraping player pages \n",
      "fetch failed for https://www.basketball-reference.com/players/d/denglu01.html -- HTTPSConnectionPool(host='www.basketball-reference.com', port=443): Read timed out. (read timeout=None)\n",
      "\n",
      "Init scraping game results for each season\n",
      "[games] 1999-00 ... 1264 games inserted\n",
      "[games] 2000-01 ... 1260 games inserted\n",
      "[games] 2001-02 ... 1260 games inserted\n",
      "[games] 2002-03 ... 1277 games inserted\n",
      "[games] 2003-04 ... 1271 games inserted\n",
      "[games] 2004-05 ... 1314 games inserted\n",
      "[games] 2005-06 ... 1319 games inserted\n",
      "[games] 2006-07 ... 1309 games inserted\n",
      "[games] 2007-08 ... 1316 games inserted\n",
      "[games] 2008-09 ... 1315 games inserted\n",
      "[games] 2009-10 ... 1312 games inserted\n",
      "[games] 2010-11 ... fetch failed for https://www.basketball-reference.com/leagues/NBA_2011_games-january.html -- HTTPSConnectionPool(host='www.basketball-reference.com', port=443): Read timed out. (read timeout=None)\n",
      "[games] 2011-12 ... [games] 2012-13 ... 1314 games inserted\n",
      "[games] 2013-14 ... 1319 games inserted\n",
      "[games] 2014-15 ... 1311 games inserted\n",
      "[games] 2015-16 ... 1316 games inserted\n",
      "[games] 2016-17 ... 1309 games inserted\n",
      "[games] 2017-18 ... 1312 games inserted\n",
      "[games] 2018-19 ... 1312 games inserted\n",
      "[games] 2019-20 ... 1143 games inserted\n",
      "[games] 2020-21 ... 1171 games inserted\n",
      "[games] 2021-22 ... 1323 games inserted\n",
      "[games] 2022-23 ... 1320 games inserted\n",
      "[games] 2023-24 ... 1319 games inserted\n",
      "[games] 2024-25 ... 1321 games inserted\n",
      "[games] 2010-11 ... 1311 games inserted\n",
      "[games] 2011-12 ... 1074 games inserted\n",
      "\n",
      "SCRAPING COMPLETE\n"
     ]
    }
   ],
   "source": [
    "MAX_RETRIES = 5\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Execute the scraping steps with retries\n",
    "def main():\n",
    "    \n",
    "    create_tables()\n",
    "    \n",
    "    permanently_failed = []\n",
    "    all_player_ids = set()\n",
    "    \n",
    "    ## Scrape team season pages to get team IDs and rosters\n",
    "    failed_seasons = {}  # season: attempt_count\n",
    "    for season in SEASONS:\n",
    "        failed_seasons[season] = 0\n",
    "\n",
    "    seasons_to_scrape = list(SEASONS)\n",
    "    team_ids_by_season = {}  # season: list of team_ids\n",
    "\n",
    "    while len(seasons_to_scrape) > 0:\n",
    "        still_failing = []\n",
    "        for season in seasons_to_scrape:\n",
    "            season_label = f\"{season - 1}-{str(season)[2:]}\"\n",
    "            try:\n",
    "                team_ids = scrape_teams_for_season(season)\n",
    "                if len(team_ids) == 0:\n",
    "                    raise RuntimeError(\"No teams found on page\")\n",
    "                team_ids_by_season[season] = team_ids\n",
    "            except Exception as e:\n",
    "                failed_seasons[season] += 1\n",
    "                if failed_seasons[season] >= MAX_RETRIES:\n",
    "                    msg = f\"season page {season_label}: {e}\"\n",
    "                    print(f\"GAVE UP on {msg}\")\n",
    "                    permanently_failed.append(msg)\n",
    "                    team_ids_by_season[season] = []\n",
    "                else:\n",
    "                    print(f\"Will retry season {season_label} (attempt {failed_seasons[season]}/{MAX_RETRIES})\")\n",
    "                    still_failing.append(season)\n",
    "        seasons_to_scrape = still_failing\n",
    "\n",
    "    # Init scrape team season pages\n",
    "    print()\n",
    "    print(f\"Init scraping team season pages for all teams and seasons\")\n",
    "    failed_team_seasons = {}  # (team_id, season): attempt_count\n",
    "    team_seasons_to_scrape = []\n",
    "\n",
    "    for season in SEASONS:\n",
    "        for team_id in team_ids_by_season.get(season, []):\n",
    "            team_seasons_to_scrape.append((team_id, season))\n",
    "            failed_team_seasons[(team_id, season)] = 0\n",
    "\n",
    "    while len(team_seasons_to_scrape) > 0:\n",
    "        still_failing = []\n",
    "        for team_id, season in team_seasons_to_scrape:\n",
    "            try:\n",
    "                scrape_team_season(team_id, season, all_player_ids)\n",
    "            except Exception as e:\n",
    "                failed_team_seasons[(team_id, season)] += 1\n",
    "                attempts = failed_team_seasons[(team_id, season)]\n",
    "                if attempts >= MAX_RETRIES:\n",
    "                    msg = f\"team {team_id} {season}: {e}\"\n",
    "                    log_row(f\"FAILED team season after {MAX_RETRIES} attempts: {team_id} {season} -- error: {e}\")\n",
    "                    permanently_failed.append(msg)\n",
    "                else:\n",
    "                    log_row(f\"failed team season: {team_id} {season} -- attempt {attempts} -- error: {e}\")\n",
    "                    still_failing.append((team_id, season))\n",
    "        team_seasons_to_scrape = still_failing\n",
    "\n",
    "    print()\n",
    "    print(f\"total unique player IDs collected from team rosters: {len(all_player_ids)}\")\n",
    "    log_row(f\"total unique player IDs collected: {len(all_player_ids)}\")\n",
    "\n",
    "    # nit scrape player pages to get player bios and season stats\n",
    "    print()\n",
    "    print(\"Init scraping player pages \")\n",
    "\n",
    "    player_list = sorted(all_player_ids)\n",
    "    failed_players = {}  # player_id: attempt_count\n",
    "\n",
    "    for pid in player_list:\n",
    "        failed_players[pid] = 0\n",
    "\n",
    "    players_to_scrape = list(player_list)\n",
    "\n",
    "    while len(players_to_scrape) > 0:\n",
    "        still_failing = []\n",
    "        for pid in players_to_scrape:\n",
    "            try:\n",
    "                attempt_num = failed_players[pid] + 1\n",
    "                log_row(f\"scraping player page: {pid} -- attempt {attempt_num}\")\n",
    "                scrape_player_page(pid)\n",
    "            except Exception as e:\n",
    "                failed_players[pid] += 1\n",
    "                attempts = failed_players[pid]\n",
    "                if attempts >= MAX_RETRIES:\n",
    "                    msg = f\"player {pid}: {e}\"\n",
    "                    log_row(f\"FAILED player after {MAX_RETRIES} attempts: {pid} -- error: {e}\")\n",
    "                    permanently_failed.append(msg)\n",
    "                else:\n",
    "                    log_row(f\"failed player: {pid} -- attempt {attempts} -- error: {e}\")\n",
    "                    still_failing.append(pid)\n",
    "        players_to_scrape = still_failing\n",
    "        if len(still_failing) > 0:\n",
    "            log_row(f\"retrying {len(still_failing)} failed players: {still_failing}\")\n",
    "\n",
    "    # Init scrape game results for each season\n",
    "    print()\n",
    "    print(\"Init scraping game results for each season\")\n",
    "\n",
    "    failed_game_seasons = {}  # season: attempt_count\n",
    "    for season in SEASONS:\n",
    "        failed_game_seasons[season] = 0\n",
    "\n",
    "    game_seasons_to_scrape = list(SEASONS)\n",
    "\n",
    "    while len(game_seasons_to_scrape) > 0:\n",
    "        still_failing = []\n",
    "        for season in game_seasons_to_scrape:\n",
    "            try:\n",
    "                scrape_games_for_season(season)\n",
    "            except Exception as e:\n",
    "                failed_game_seasons[season] += 1\n",
    "                attempts = failed_game_seasons[season]\n",
    "                season_label = f\"{season - 1}-{str(season)[2:]}\"\n",
    "                if attempts >= MAX_RETRIES:\n",
    "                    msg = f\"games {season_label}: {e}\"\n",
    "                    log_row(f\"FAILED games for season after {MAX_RETRIES} attempts: {season_label} -- error: {e}\")\n",
    "                    permanently_failed.append(msg)\n",
    "                else:\n",
    "                    log_row(f\"failed games for season {season_label} -- attempt {attempts} -- error: {e}\")\n",
    "                    still_failing.append(season)\n",
    "        game_seasons_to_scrape = still_failing\n",
    "\n",
    "    print()\n",
    "    print(\"SCRAPING COMPLETE\")\n",
    "\n",
    "    if len(permanently_failed) == 0:\n",
    "        log_row(\"everything succeeded, no failures\")\n",
    "    else:\n",
    "        log_row(f\"{len(permanently_failed)} item(s) failed all {MAX_RETRIES} attempts:\")\n",
    "        for item in permanently_failed:\n",
    "            log_row(f\"  * {item}\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9000001",
   "metadata": {},
   "source": [
    "## 9. Validation\n",
    "\n",
    "Run queries to verify the database was populated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row counts:\n",
      "  players: 2,551\n",
      "  teams: 37\n",
      "  player_season_stats: 12,667\n",
      "  team_season_stats: 775\n",
      "  games: 33,392\n",
      "\n",
      "spot check -- LeBron James (jamesle01):\n",
      "  name: LeBron James\n",
      "  born: 1984-12-30\n",
      "  height/weight: 6-9, 250 lb\n",
      "  position: Small Forward, Power Forward, Point Guard, Center, and Shooting Guard, shoots: Right\n",
      "  draft: Cleveland Cavaliers / round 1 / pick 1 / 2003\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "no such column: season",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdone validating\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43mvalidate_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mvalidate_database\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  not found in db\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT season, team_id, points, rebounds, assists\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFROM player_season_stats \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWHERE player_id = \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjamesle01\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mORDER BY season\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     33\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m lebron_seasons = cursor.fetchall()\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  seasons on file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(lebron_seasons)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOperationalError\u001b[39m: no such column: season"
     ]
    }
   ],
   "source": [
    "# Run a few validation queries against the database to spot check results\n",
    "def validate_database():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # row counts\n",
    "    print(\"row counts:\")\n",
    "    tables = [\"players\", \"teams\", \"player_season_stats\", \"team_season_stats\", \"games\"]\n",
    "    for table_name in tables:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"  {table_name}: {count:,}\")\n",
    "\n",
    "    # spot check lebron\n",
    "    print()\n",
    "    print(\"spot check -- LeBron James (jamesle01):\")\n",
    "    cursor.execute(\"SELECT * FROM players WHERE player_id = 'jamesle01'\")\n",
    "    row = cursor.fetchone()\n",
    "    if row:\n",
    "        print(f\"  name: {row[1]}\")\n",
    "        print(f\"  born: {row[2]}\")\n",
    "        print(f\"  height/weight: {row[3]}, {row[4]} lb\")\n",
    "        print(f\"  position: {row[5]}, shoots: {row[6]}\")\n",
    "        print(f\"  draft: {row[10]} / round {row[8]} / pick {row[9]} / {row[7]}\")\n",
    "    else:\n",
    "        print(\"  not found in db\")\n",
    "\n",
    "    cursor.execute(\n",
    "        \"SELECT season, team_id, points, rebounds, assists\"\n",
    "        \"FROM player_season_stats \"\n",
    "        \"WHERE player_id = 'jamesle01' \"\n",
    "        \"ORDER BY season\"\n",
    "    )\n",
    "    lebron_seasons = cursor.fetchall()\n",
    "    print(f\"  seasons on file: {len(lebron_seasons)}\")\n",
    "    for s in lebron_seasons:\n",
    "        print(f\"    {s[0]} ({s[1]}): {s[2]} ppg / {s[3]} rpg / {s[4]} apg\")\n",
    "\n",
    "    # spot check lakers 2020\n",
    "    print()\n",
    "    print(\"spot check -- LAL 2019-20:\")\n",
    "    cursor.execute(\n",
    "        \"SELECT * FROM team_season_stats WHERE team_id = 'LAL' AND season = 2020\"\n",
    "    )\n",
    "    row = cursor.fetchone()\n",
    "    if row:\n",
    "        print(f\"  record: {row[2]}-{row[3]} (win%: {row[4]})\")\n",
    "        print(f\"  pace: {row[5]}, ortg: {row[6]}, drtg: {row[7]}, srs: {row[8]}\")\n",
    "    else:\n",
    "        print(\"  not found in db\")\n",
    "\n",
    "    # games check\n",
    "    print()\n",
    "    print(\"spot check -- games:\")\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM games WHERE season = 2024\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"  2023-24 season: {count} games\")\n",
    "\n",
    "    cursor.execute(\"SELECT * FROM games ORDER BY game_date LIMIT 5\")\n",
    "    sample_games = cursor.fetchall()\n",
    "    print(f\"  first 5 games in db:\")\n",
    "    for game in sample_games:\n",
    "        winner = \"home W\" if game[7] else \"away W\"\n",
    "        print(f\"    {game[2]} -- {game[4]} at {game[3]}, {game[6]}-{game[5]} ({winner})\")\n",
    "\n",
    "    # season coverage\n",
    "    print()\n",
    "    cursor.execute(\"SELECT DISTINCT season FROM team_season_stats ORDER BY season\")\n",
    "    seasons_found = [r[0] for r in cursor.fetchall()]\n",
    "    if seasons_found:\n",
    "        print(f\"seasons in team_season_stats: {seasons_found[0]} to {seasons_found[-1]} ({len(seasons_found)} total)\")\n",
    "    else:\n",
    "        print(\"no seasons found in team_season_stats\")\n",
    "\n",
    "    conn.close()\n",
    "    print()\n",
    "    print(\"done validating\")\n",
    "\n",
    "\n",
    "validate_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39471d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
