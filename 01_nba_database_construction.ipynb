{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# NBA Database Scraper (2000-Present)\n",
    "\n",
    "This notebook scrapes Basketball Reference to build a SQLite database of NBA data.\n",
    "\n",
    "**Tables:**\n",
    "- `players` — player biographical info\n",
    "- `teams` — team identifiers and conference/division\n",
    "- `player_season_stats` — per-game and advanced stats by season\n",
    "- `team_season_stats` — team record and ratings by season\n",
    "- `games` — individual game results\n",
    "\n",
    "**Seasons:** 1999-2000 through 2024-25 (referenced as 2000-2025 in Basketball Reference URLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kevin Schroeder\n",
    "# Libraries: requests, BeautifulSoup, sqlite3\n",
    "# Source: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import sqlite3\n",
    "import time\n",
    "import re\n",
    "\n",
    "DB_PATH = \"nba.db\"\n",
    "BASE_URL = \"https://www.basketball-reference.com\"\n",
    "DELAY = 3\n",
    "\n",
    "# Season range\n",
    "SEASONS = list(range(2000, 2026))\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Config loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. Database Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kevin Schroeder\n",
    "# Creates all five database tables with primary keys and data types.\n",
    "# Source: https://www.sqlite.org/docs.html\n",
    "\n",
    "def create_tables():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS players (\n",
    "            player_id TEXT PRIMARY KEY,\n",
    "            full_name TEXT,\n",
    "            birth_date TEXT,\n",
    "            height TEXT,\n",
    "            weight INTEGER,\n",
    "            position TEXT,\n",
    "            shoots TEXT,\n",
    "            draft_year INTEGER,\n",
    "            draft_round INTEGER,\n",
    "            draft_pick INTEGER,\n",
    "            draft_team TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS teams (\n",
    "            team_id TEXT PRIMARY KEY,\n",
    "            team_name TEXT,\n",
    "            conference TEXT,\n",
    "            division TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS player_season_stats (\n",
    "            player_id TEXT,\n",
    "            season INTEGER,\n",
    "            team_id TEXT,\n",
    "            games INTEGER,\n",
    "            games_started INTEGER,\n",
    "            minutes REAL,\n",
    "            points REAL,\n",
    "            rebounds REAL,\n",
    "            assists REAL,\n",
    "            per REAL,\n",
    "            ts_pct REAL,\n",
    "            ws REAL,\n",
    "            bpm REAL,\n",
    "            vorp REAL,\n",
    "            PRIMARY KEY (player_id, season, team_id)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS team_season_stats (\n",
    "            team_id TEXT,\n",
    "            season INTEGER,\n",
    "            wins INTEGER,\n",
    "            losses INTEGER,\n",
    "            win_pct REAL,\n",
    "            pace REAL,\n",
    "            offensive_rating REAL,\n",
    "            defensive_rating REAL,\n",
    "            srs REAL,\n",
    "            PRIMARY KEY (team_id, season)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS games (\n",
    "            game_id TEXT PRIMARY KEY,\n",
    "            season INTEGER,\n",
    "            game_date TEXT,\n",
    "            home_team TEXT,\n",
    "            away_team TEXT,\n",
    "            home_score INTEGER,\n",
    "            away_score INTEGER,\n",
    "            home_win INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"tables created\")\n",
    "\n",
    "\n",
    "create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000001",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kevin Schroeder\n",
    "# Helper functions for fetching pages, parsing HTML tables, and safe type conversion.\n",
    "# Source: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "# Use BeautifulSoup to fetch and parse pages\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        time.sleep(DELAY)\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"fetch failed for {url} -- {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Helper functions for parsing tables using BeautifulSoup\n",
    "def find_table(soup, table_id):\n",
    "    # Try normal search first\n",
    "    table = soup.find(\"table\", {\"id\": table_id})\n",
    "    if table is not None:\n",
    "        return table\n",
    "\n",
    "    # Search inside HTML comments\n",
    "    # Comments are not parsed as part of the normal DOM, so we have to look through them manually\n",
    "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        comment_text = str(comment)\n",
    "        if table_id not in comment_text:\n",
    "            continue\n",
    "        comment_soup = BeautifulSoup(comment_text, \"html.parser\")\n",
    "        table = comment_soup.find(\"table\", {\"id\": table_id})\n",
    "        if table is not None:\n",
    "            return table\n",
    "\n",
    "    return None\n",
    "\n",
    "# Converts string to int or None\n",
    "def safe_int(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    value = str(value).strip().replace(\",\", \"\")\n",
    "    if value == \"\" or value == \"-\":\n",
    "        return None\n",
    "    try:\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Converts string to float or None\n",
    "def safe_float(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    value = str(value).strip().replace(\",\", \"\")\n",
    "    if value == \"\" or value == \"-\":\n",
    "        return None\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Extracts text content of a table cell\n",
    "def get_cell_text(row, stat_name):\n",
    "    cell = row.find(\"td\", {\"data-stat\": stat_name})\n",
    "    if cell is None:\n",
    "        cell = row.find(\"th\", {\"data-stat\": stat_name})\n",
    "    if cell is None:\n",
    "        return \"\"\n",
    "    return cell.get_text(strip=True)\n",
    "\n",
    "\n",
    "# Extracts links from a table cell\n",
    "def get_cell_link(row, stat_name):\n",
    "    cell = row.find(\"td\", {\"data-stat\": stat_name})\n",
    "    if cell is None:\n",
    "        cell = row.find(\"th\", {\"data-stat\": stat_name})\n",
    "    if cell is None:\n",
    "        return None\n",
    "    link = cell.find(\"a\")\n",
    "    if link is None:\n",
    "        return None\n",
    "    return link.get(\"href\", None)\n",
    "\n",
    "# Prints row to log file\n",
    "# Prevents cell outputs from being too long\n",
    "def log_row(row):\n",
    "    with open(\"scrape_log.txt\", \"a\") as f:\n",
    "        f.write(str(row) + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"Helpers ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. Scrape Teams from Season Pages\n",
    "\n",
    "For each season, load the season summary page and extract the team names,\n",
    "IDs, conferences, and divisions from the standings tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kevin Schroeder\n",
    "# Scrapes team names, IDs, conferences, and divisions from season standings pages.\n",
    "# Data source: https://www.basketball-reference.com\n",
    "\n",
    "def scrape_teams_for_season(season):\n",
    "    url = f\"{BASE_URL}/leagues/NBA_{season}.html\"\n",
    "    season_label = f\"{season - 1}-{str(season)[2:]}\"\n",
    "    print(f\"[teams]: {season_label}\")\n",
    "    log_row(f\"scraping teams for season: {season_label}\")\n",
    "\n",
    "    soup = get_soup(url)\n",
    "    if soup is None:\n",
    "        raise RuntimeError(f\"Failed to fetch {url}\")\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    team_ids = []\n",
    "\n",
    "    # Basketball Reference has Eastern and Western conference standings.\n",
    "    # Table IDs vary by year; try multiple possibilities.\n",
    "    conference_tables = [\n",
    "        (\"E\", \"Eastern\"),\n",
    "        (\"W\", \"Western\"),\n",
    "    ]\n",
    "\n",
    "    for conf_code, conf_name in conference_tables:\n",
    "        table = find_table(soup, f\"divs_standings_{conf_code}\")\n",
    "        if table is None:\n",
    "            table = find_table(soup, f\"confs_standings_{conf_code}\")\n",
    "\n",
    "        if table is None:\n",
    "            print(f\"(missing {conf_name} standings) \", end=\"\")\n",
    "            continue\n",
    "\n",
    "        # Division headers are interspersed in the table, \n",
    "        # so track the current division as we go\n",
    "        current_division = \"\"\n",
    "        all_rows = table.find_all(\"tr\")\n",
    "\n",
    "        for row in all_rows:\n",
    "            header_th = row.find(\"th\", {\"colspan\": True})\n",
    "            if header_th:\n",
    "                current_division = header_th.get_text(strip=True)\n",
    "                continue\n",
    "\n",
    "            team_cell = row.find(\"th\", {\"data-stat\": \"team_name\"})\n",
    "            if team_cell is None:\n",
    "                team_cell = row.find(\"td\", {\"data-stat\": \"team_name\"})\n",
    "            if team_cell is None:\n",
    "                continue\n",
    "\n",
    "            link = team_cell.find(\"a\")\n",
    "            if link is None:\n",
    "                continue\n",
    "\n",
    "            href = link.get(\"href\", \"\")\n",
    "            team_name = link.get_text(strip=True)\n",
    "\n",
    "            # Parse team_id from URL like /teams/LAL/2020.html\n",
    "            parts = href.strip(\"/\").split(\"/\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            team_id = parts[1]\n",
    "\n",
    "            team_ids.append(team_id)\n",
    "\n",
    "            cursor.execute(\n",
    "                \"INSERT OR IGNORE INTO teams (team_id, team_name, conference, division) \"\n",
    "                \"VALUES (?, ?, ?, ?)\",\n",
    "                (team_id, team_name, conf_name, current_division),\n",
    "            )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"found {len(team_ids)} teams\")\n",
    "    return team_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. Scrape Team Season Stats\n",
    "\n",
    "For each team-season, load the team page and extract:\n",
    "- Win/loss record from the page meta section\n",
    "- Advanced stats (Pace, ORtg, DRtg, SRS) from the team_misc table\n",
    "- Roster player links (to build the set of player IDs to scrape later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kevin Schroeder\n",
    "# Scrapes team season stats (wins, losses, pace, ratings, SRS) and collects roster player IDs.\n",
    "# Data source: https://www.basketball-reference.com\n",
    "\n",
    "def scrape_team_season(team_id, season, all_player_ids):\n",
    "    url = f\"{BASE_URL}/teams/{team_id}/{season}.html\"\n",
    "    print(f\"{team_id} {season} ...\", end=\" \")\n",
    "    log_row(f\"scraping team season: {team_id} {season}\")\n",
    "\n",
    "    soup = get_soup(url)\n",
    "    if soup is None:\n",
    "        raise RuntimeError(f\"Failed to fetch {url}\")\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    wins = None\n",
    "    losses = None\n",
    "\n",
    "    meta_div = soup.find(\"div\", {\"id\": \"meta\"})\n",
    "    if meta_div:\n",
    "        meta_paragraphs = meta_div.find_all(\"p\")\n",
    "        for paragraph in meta_paragraphs:\n",
    "            paragraph_text = paragraph.get_text()\n",
    "            # Look for a line containing the record\n",
    "            has_record_keyword = (\"Record\" in paragraph_text or \"record\" in paragraph_text)\n",
    "            record_match = re.search(r'(\\d+)-(\\d+)', paragraph_text)\n",
    "            if record_match and has_record_keyword:\n",
    "                wins = safe_int(record_match.group(1))\n",
    "                losses = safe_int(record_match.group(2))\n",
    "                break\n",
    "\n",
    "    # Compute win percentage\n",
    "    win_pct = None\n",
    "    if wins is not None and losses is not None:\n",
    "        total_games = wins + losses\n",
    "        if total_games > 0:\n",
    "            win_pct = round(wins / total_games, 3)\n",
    "\n",
    "    # Extract advanced team stats from team_misc table\n",
    "    pace = None\n",
    "    offensive_rating = None\n",
    "    defensive_rating = None\n",
    "    srs = None\n",
    "\n",
    "    misc_table = find_table(soup, \"team_misc\")\n",
    "    if misc_table:\n",
    "        # The table has thead (column headers) and tbody (data rows).\n",
    "        # We only want tbody rows -- otherwise we match the header text\n",
    "        # like \"Pace\" instead of the actual value like \"89.7\".\n",
    "        tbody = misc_table.find(\"tbody\")\n",
    "        if tbody:\n",
    "            data_rows = tbody.find_all(\"tr\")\n",
    "        else:\n",
    "            data_rows = misc_table.find_all(\"tr\")\n",
    "\n",
    "        for row in data_rows:\n",
    "            pace_text = get_cell_text(row, \"pace\")\n",
    "            if pace_text == \"\":\n",
    "                continue\n",
    "            pace_val = safe_float(pace_text)\n",
    "            if pace_val is None:\n",
    "                # this is a non-numeric row (like \"Lg Rank\"), skip it\n",
    "                continue\n",
    "            pace = pace_val\n",
    "            srs = safe_float(get_cell_text(row, \"srs\"))\n",
    "            offensive_rating = safe_float(get_cell_text(row, \"off_rtg\"))\n",
    "            defensive_rating = safe_float(get_cell_text(row, \"def_rtg\"))\n",
    "            break\n",
    "\n",
    "    cursor.execute(\n",
    "        \"INSERT OR REPLACE INTO team_season_stats \"\n",
    "        \"(team_id, season, wins, losses, win_pct, pace, \"\n",
    "        \"offensive_rating, defensive_rating, srs) \"\n",
    "        \"VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "        (team_id, season, wins, losses, win_pct, pace,\n",
    "         offensive_rating, defensive_rating, srs),\n",
    "    )\n",
    "\n",
    "    # Extract roster player IDs\n",
    "    roster_table = find_table(soup, \"roster\")\n",
    "    roster_count = 0\n",
    "    if roster_table:\n",
    "        roster_rows = roster_table.find_all(\"tr\")\n",
    "        for row in roster_rows:\n",
    "            links = row.find_all(\"a\")\n",
    "            for link in links:\n",
    "                href = link.get(\"href\", \"\")\n",
    "                if \"/players/\" in href and href.endswith(\".html\"):\n",
    "                    parts = href.strip(\"/\").split(\"/\")\n",
    "                    if len(parts) >= 3:\n",
    "                        player_id = parts[2].replace(\".html\", \"\")\n",
    "                        all_player_ids.add(player_id)\n",
    "                        roster_count += 1\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    log_row(f\"team season: {team_id} {season} -- wins: {wins}, losses: {losses}, win%: {win_pct}, pace: {pace}, off_rtg: {offensive_rating}, def_rtg: {defensive_rating}, srs: {srs}, num_players: {roster_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. Scrape Player Pages (Bio + Season Stats)\n",
    "\n",
    "For each player, load their page once and extract:\n",
    "- Biographical info (name, height, weight, position, draft) from the meta section\n",
    "- Per-game stats from the `per_game` table\n",
    "- Advanced stats (PER, TS%, WS, BPM, VORP) from the `advanced` table\n",
    "\n",
    "The two stat tables are merged by (season, team_id) key before inserting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kevin Schroeder\n",
    "# Scrapes player bio (name, height, weight, position, draft) and season stats\n",
    "# (per-game and advanced) from individual player pages.\n",
    "# Data source: https://www.basketball-reference.com\n",
    "\n",
    "def scrape_player_page(player_id):\n",
    "    # Player pages are organized by the first letter of the player_id\n",
    "    first_letter = player_id[0]\n",
    "    url = f\"{BASE_URL}/players/{first_letter}/{player_id}.html\"\n",
    "\n",
    "    soup = get_soup(url)\n",
    "    if soup is None:\n",
    "        raise RuntimeError(f\"Failed to fetch {url}\")\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    ## Player bio and draft info\n",
    "\n",
    "    full_name = \"\"\n",
    "    birth_date = None\n",
    "    height = None\n",
    "    weight = None\n",
    "    position = None\n",
    "    shoots = None\n",
    "    draft_year = None\n",
    "    draft_round = None\n",
    "    draft_pick = None\n",
    "    draft_team = None\n",
    "\n",
    "    # Name\n",
    "    heading = soup.find(\"h1\")\n",
    "    if heading:\n",
    "        name_span = heading.find(\"span\")\n",
    "        if name_span:\n",
    "            full_name = name_span.get_text(strip=True)\n",
    "        else:\n",
    "            full_name = heading.get_text(strip=True)\n",
    "\n",
    "    # Meta section fields\n",
    "    meta_div = soup.find(\"div\", {\"id\": \"meta\"})\n",
    "    if meta_div:\n",
    "        paragraphs = meta_div.find_all(\"p\")\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            text = paragraph.get_text(strip=True)\n",
    "\n",
    "            # Position\n",
    "            if \"Position:\" in text:\n",
    "                pos_match = re.search(\n",
    "                    r'Position:\\s*(.+?)(?:\\s*\\u25aa|\\s*Shoots:|$)', text\n",
    "                )\n",
    "                if pos_match:\n",
    "                    position = pos_match.group(1).strip()\n",
    "\n",
    "            # Shooting hand\n",
    "            if \"Shoots:\" in text:\n",
    "                shoots_match = re.search(r'Shoots:\\s*(\\w+)', text)\n",
    "                if shoots_match:\n",
    "                    shoots = shoots_match.group(1).strip()\n",
    "\n",
    "            # Height / weight\n",
    "            height_match = re.search(r'(\\d+-\\d+)', text)\n",
    "            weight_match = re.search(r'(\\d+)lb', text)\n",
    "            if height_match and (\"lb\" in text or \"cm\" in text):\n",
    "                height = height_match.group(1)\n",
    "            if weight_match:\n",
    "                weight = safe_int(weight_match.group(1))\n",
    "\n",
    "            # Draft info\n",
    "            if \"Draft:\" in text:\n",
    "                draft_team_match = re.search(r'Draft:\\s*(.+?),', text)\n",
    "                if draft_team_match:\n",
    "                    draft_team = draft_team_match.group(1).strip()\n",
    "\n",
    "                round_match = re.search(r'(\\d+)\\w*\\s*round', text)\n",
    "                if round_match:\n",
    "                    draft_round = safe_int(round_match.group(1))\n",
    "\n",
    "                pick_match = re.search(r'(\\d+)\\w*\\s*pick', text)\n",
    "                if pick_match:\n",
    "                    draft_pick = safe_int(pick_match.group(1))\n",
    "\n",
    "                year_match = re.search(r'(\\d{4})\\s*NBA\\s*Draft', text)\n",
    "                if year_match:\n",
    "                    draft_year = safe_int(year_match.group(1))\n",
    "\n",
    "        # Birth date\n",
    "        birth_span = meta_div.find(\"span\", {\"id\": \"necro-birth\"})\n",
    "        if birth_span:\n",
    "            birth_date = birth_span.get(\"data-birth\", None)\n",
    "            if birth_date is None:\n",
    "                birth_date = birth_span.get_text(strip=True)\n",
    "\n",
    "    cursor.execute(\n",
    "        \"INSERT OR REPLACE INTO players \"\n",
    "        \"(player_id, full_name, birth_date, height, weight, position, shoots, \"\n",
    "        \"draft_year, draft_round, draft_pick, draft_team) \"\n",
    "        \"VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "        (player_id, full_name, birth_date, height, weight, position, shoots,\n",
    "         draft_year, draft_round, draft_pick, draft_team),\n",
    "    )\n",
    "\n",
    "    ## Player season stats\n",
    "    \n",
    "    ### Per-game season stats\n",
    "    per_game_data = {}  # (season_year, team_id): {games, games_started, minutes, points, rebounds, assists}\n",
    "\n",
    "    # Basketball Reference uses table id \"per_game_stats\" (not \"per_game\")\n",
    "    per_game_table = find_table(soup, \"per_game_stats\")\n",
    "    if per_game_table:\n",
    "        tbody = per_game_table.find(\"tbody\")\n",
    "        rows = tbody.find_all(\"tr\") if tbody else per_game_table.find_all(\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            # Skip sub-header rows\n",
    "            row_classes = row.get(\"class\", [])\n",
    "            if \"thead\" in row_classes or \"partial_table\" in row_classes:\n",
    "                continue\n",
    "\n",
    "            # Parse season text like \"2019-20\" -> season_year = 2020\n",
    "            # Basketball Reference uses data-stat=\"year_id\" for the season column\n",
    "            season_text = get_cell_text(row, \"year_id\")\n",
    "            if season_text == \"\":\n",
    "                continue\n",
    "            season_match = re.match(r'(\\d{4})-(\\d{2})', season_text)\n",
    "            if season_match is None:\n",
    "                continue\n",
    "            season_year = int(season_match.group(1)) + 1\n",
    "\n",
    "            if season_year < 2000 or season_year > 2025:\n",
    "                continue\n",
    "\n",
    "            # Get team -- data-stat is \"team_name_abbr\" on player pages\n",
    "            team_text = get_cell_text(row, \"team_name_abbr\")\n",
    "            if team_text == \"TOT\":\n",
    "                continue\n",
    "\n",
    "            team_link = get_cell_link(row, \"team_name_abbr\")\n",
    "            if team_link:\n",
    "                link_parts = team_link.strip(\"/\").split(\"/\")\n",
    "                team_id_val = link_parts[1] if len(link_parts) >= 2 else team_text\n",
    "            else:\n",
    "                team_id_val = team_text\n",
    "\n",
    "            if team_id_val == \"\":\n",
    "                continue\n",
    "\n",
    "            # data-stat names: \"games\", \"games_started\" (not \"g\", \"gs\")\n",
    "            games = safe_int(get_cell_text(row, \"games\"))\n",
    "            games_started = safe_int(get_cell_text(row, \"games_started\"))\n",
    "            minutes = safe_float(get_cell_text(row, \"mp_per_g\"))\n",
    "            points = safe_float(get_cell_text(row, \"pts_per_g\"))\n",
    "            rebounds = safe_float(get_cell_text(row, \"trb_per_g\"))\n",
    "            assists = safe_float(get_cell_text(row, \"ast_per_g\"))\n",
    "\n",
    "            key = (season_year, team_id_val)\n",
    "            per_game_data[key] = {\n",
    "                \"games\": games,\n",
    "                \"games_started\": games_started,\n",
    "                \"minutes\": minutes,\n",
    "                \"points\": points,\n",
    "                \"rebounds\": rebounds,\n",
    "                \"assists\": assists,\n",
    "            }\n",
    "\n",
    "    ### Advanced season stats\n",
    "    advanced_data = {}  # (season_year, team_id): {per, ts_pct, ws, bpm, vorp}\n",
    "\n",
    "    advanced_table = find_table(soup, \"advanced\")\n",
    "    if advanced_table:\n",
    "        tbody = advanced_table.find(\"tbody\")\n",
    "        rows = tbody.find_all(\"tr\") if tbody else advanced_table.find_all(\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            row_classes = row.get(\"class\", [])\n",
    "            if \"thead\" in row_classes or \"partial_table\" in row_classes:\n",
    "                continue\n",
    "\n",
    "            season_text = get_cell_text(row, \"year_id\")\n",
    "            if season_text == \"\":\n",
    "                continue\n",
    "            season_match = re.match(r'(\\d{4})-(\\d{2})', season_text)\n",
    "            if season_match is None:\n",
    "                continue\n",
    "            season_year = int(season_match.group(1)) + 1\n",
    "\n",
    "            if season_year < 2000 or season_year > 2025:\n",
    "                continue\n",
    "\n",
    "            team_text = get_cell_text(row, \"team_name_abbr\")\n",
    "            if team_text == \"TOT\":\n",
    "                continue\n",
    "\n",
    "            team_link = get_cell_link(row, \"team_name_abbr\")\n",
    "            if team_link:\n",
    "                link_parts = team_link.strip(\"/\").split(\"/\")\n",
    "                team_id_val = link_parts[1] if len(link_parts) >= 2 else team_text\n",
    "            else:\n",
    "                team_id_val = team_text\n",
    "\n",
    "            per_val = safe_float(get_cell_text(row, \"per\"))\n",
    "            ts_pct = safe_float(get_cell_text(row, \"ts_pct\"))\n",
    "            ws = safe_float(get_cell_text(row, \"ws\"))\n",
    "            bpm = safe_float(get_cell_text(row, \"bpm\"))\n",
    "            vorp = safe_float(get_cell_text(row, \"vorp\"))\n",
    "\n",
    "            key = (season_year, team_id_val)\n",
    "            advanced_data[key] = {\n",
    "                \"per\": per_val,\n",
    "                \"ts_pct\": ts_pct,\n",
    "                \"ws\": ws,\n",
    "                \"bpm\": bpm,\n",
    "                \"vorp\": vorp,\n",
    "            }\n",
    "\n",
    "    ### Merge per-game and advanced, then insert\n",
    "    all_keys = set(per_game_data.keys()) | set(advanced_data.keys())\n",
    "\n",
    "    for key in all_keys:\n",
    "        season_year, team_id_val = key\n",
    "\n",
    "        pg = per_game_data.get(key, {})\n",
    "        adv = advanced_data.get(key, {})\n",
    "\n",
    "        cursor.execute(\n",
    "            \"INSERT OR REPLACE INTO player_season_stats \"\n",
    "            \"(player_id, season, team_id, games, games_started, minutes, \"\n",
    "            \"points, rebounds, assists, per, ts_pct, ws, bpm, vorp) \"\n",
    "            \"VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (\n",
    "                player_id,\n",
    "                season_year,\n",
    "                team_id_val,\n",
    "                pg.get(\"games\"),\n",
    "                pg.get(\"games_started\"),\n",
    "                pg.get(\"minutes\"),\n",
    "                pg.get(\"points\"),\n",
    "                pg.get(\"rebounds\"),\n",
    "                pg.get(\"assists\"),\n",
    "                adv.get(\"per\"),\n",
    "                adv.get(\"ts_pct\"),\n",
    "                adv.get(\"ws\"),\n",
    "                adv.get(\"bpm\"),\n",
    "                adv.get(\"vorp\"),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    log_row(f\"player: {full_name} -- {len(all_keys)} season entries inserted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## 7. Scrape Games\n",
    "\n",
    "For each season, load the schedule page and follow the monthly sub-links.\n",
    "Each month page contains a schedule table with game results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kevin Schroeder\n",
    "# Scrapes game results (date, teams, scores, winner) from monthly schedule pages.\n",
    "# Data source: https://www.basketball-reference.com\n",
    "\n",
    "def scrape_games_for_season(season):\n",
    "    # Games are organized by season on a main schedule page, \n",
    "    # which links to monthly sub-pages for some seasons.\n",
    "    url = f\"{BASE_URL}/leagues/NBA_{season}_games.html\"\n",
    "    season_label = f\"{season - 1}-{str(season)[2:]}\"\n",
    "    print(f\"[games] {season_label} ...\", end=\" \")\n",
    "    log_row(f\"scraping games for season: {season_label}\")\n",
    "\n",
    "    soup = get_soup(url)\n",
    "    if soup is None:\n",
    "        raise RuntimeError(f\"Failed to fetch {url}\")\n",
    "\n",
    "    # Collect monthly schedule page links\n",
    "    month_links = []\n",
    "    filter_div = soup.find(\"div\", {\"class\": \"filter\"})\n",
    "    if filter_div:\n",
    "        links = filter_div.find_all(\"a\")\n",
    "        for link in links:\n",
    "            href = link.get(\"href\", \"\")\n",
    "            if \"games\" in href:\n",
    "                full_url = BASE_URL + href\n",
    "                month_links.append(full_url)\n",
    "\n",
    "    # If no month links found, the current page may have the schedule directly\n",
    "    if len(month_links) == 0:\n",
    "        month_links = [url]\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    games_inserted = 0\n",
    "\n",
    "    for month_url in month_links:\n",
    "        # Avoid re-fetching the page we already have\n",
    "        if month_url == url:\n",
    "            month_soup = soup\n",
    "        else:\n",
    "            month_soup = get_soup(month_url)\n",
    "\n",
    "        if month_soup is None:\n",
    "            raise RuntimeError(f\"Failed to fetch month page {month_url}\")\n",
    "\n",
    "        schedule_table = find_table(month_soup, \"schedule\")\n",
    "        if schedule_table is None:\n",
    "            continue\n",
    "\n",
    "        tbody = schedule_table.find(\"tbody\")\n",
    "        rows = tbody.find_all(\"tr\") if tbody else schedule_table.find_all(\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            # Skip header rows\n",
    "            row_classes = row.get(\"class\", [])\n",
    "            if \"thead\" in row_classes:\n",
    "                continue\n",
    "\n",
    "            # Box score link gives us the game_id\n",
    "            box_link = get_cell_link(row, \"box_score_text\")\n",
    "            if box_link is None:\n",
    "                continue\n",
    "\n",
    "            # Parse game_id from box link (i.e. /boxscores/202001010LAL.html)\n",
    "            game_id_with_ext = box_link.split(\"/\")[-1]\n",
    "            game_id = game_id_with_ext.replace(\".html\", \"\")\n",
    "\n",
    "            # Date\n",
    "            date_cell = row.find(\"th\", {\"data-stat\": \"date_game\"})\n",
    "            if date_cell is None:\n",
    "                continue\n",
    "            game_date = date_cell.get(\"csk\", date_cell.get_text(strip=True))\n",
    "\n",
    "            # Away team\n",
    "            away_link = get_cell_link(row, \"visitor_team_name\")\n",
    "            away_team_id = \"\"\n",
    "            if away_link:\n",
    "                parts = away_link.strip(\"/\").split(\"/\")\n",
    "                if len(parts) >= 2:\n",
    "                    away_team_id = parts[1]\n",
    "\n",
    "            # Home team\n",
    "            home_link = get_cell_link(row, \"home_team_name\")\n",
    "            home_team_id = \"\"\n",
    "            if home_link:\n",
    "                parts = home_link.strip(\"/\").split(\"/\")\n",
    "                if len(parts) >= 2:\n",
    "                    home_team_id = parts[1]\n",
    "\n",
    "            # Scores\n",
    "            away_score = safe_int(get_cell_text(row, \"visitor_pts\"))\n",
    "            home_score = safe_int(get_cell_text(row, \"home_pts\"))\n",
    "\n",
    "            # Skip rows with missing scores (e.g., future games)\n",
    "            if home_score is None or away_score is None:\n",
    "                continue\n",
    "\n",
    "            home_win = 1 if home_score > away_score else 0\n",
    "\n",
    "            cursor.execute(\n",
    "                \"INSERT OR REPLACE INTO games \"\n",
    "                \"(game_id, season, game_date, home_team, away_team, \"\n",
    "                \"home_score, away_score, home_win) \"\n",
    "                \"VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "                (game_id, season, game_date, home_team_id, away_team_id,\n",
    "                 home_score, away_score, home_win),\n",
    "            )\n",
    "            games_inserted += 1\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"{games_inserted} games inserted\")\n",
    "    log_row(f\"total games inserted for season {season_label}: {games_inserted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000001",
   "metadata": {},
   "source": [
    "## 8. Main Orchestration\n",
    "\n",
    "Run the full scraping pipeline:\n",
    "1. Create tables\n",
    "2. For each season: scrape teams, then scrape each team's season page (collecting player IDs)\n",
    "3. Scrape all discovered player pages\n",
    "4. Scrape game results for each season\n",
    "\n",
    "Each step uses `INSERT OR REPLACE`, so the script can be safely re-run if interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kevin Schroeder\n",
    "# Main orchestration: runs the full scraping pipeline with retry logic.\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Execute the scraping steps with retries\n",
    "def main():\n",
    "    \n",
    "    create_tables()\n",
    "    \n",
    "    permanently_failed = []\n",
    "    all_player_ids = set()\n",
    "    \n",
    "    ## Scrape team season pages to get team IDs and rosters\n",
    "    failed_seasons = {}  # season: attempt_count\n",
    "    for season in SEASONS:\n",
    "        failed_seasons[season] = 0\n",
    "\n",
    "    seasons_to_scrape = list(SEASONS)\n",
    "    team_ids_by_season = {}  # season: list of team_ids\n",
    "\n",
    "    while len(seasons_to_scrape) > 0:\n",
    "        still_failing = []\n",
    "        for season in seasons_to_scrape:\n",
    "            season_label = f\"{season - 1}-{str(season)[2:]}\"\n",
    "            try:\n",
    "                team_ids = scrape_teams_for_season(season)\n",
    "                if len(team_ids) == 0:\n",
    "                    raise RuntimeError(\"No teams found on page\")\n",
    "                team_ids_by_season[season] = team_ids\n",
    "            except Exception as e:\n",
    "                failed_seasons[season] += 1\n",
    "                if failed_seasons[season] >= MAX_RETRIES:\n",
    "                    msg = f\"season page {season_label}: {e}\"\n",
    "                    print(f\"GAVE UP on {msg}\")\n",
    "                    permanently_failed.append(msg)\n",
    "                    team_ids_by_season[season] = []\n",
    "                else:\n",
    "                    print(f\"Will retry season {season_label} (attempt {failed_seasons[season]}/{MAX_RETRIES})\")\n",
    "                    still_failing.append(season)\n",
    "        seasons_to_scrape = still_failing\n",
    "\n",
    "    # Init scrape team season pages\n",
    "    print()\n",
    "    print(f\"Init scraping team season pages for all teams and seasons\")\n",
    "    failed_team_seasons = {}  # (team_id, season): attempt_count\n",
    "    team_seasons_to_scrape = []\n",
    "\n",
    "    for season in SEASONS:\n",
    "        for team_id in team_ids_by_season.get(season, []):\n",
    "            team_seasons_to_scrape.append((team_id, season))\n",
    "            failed_team_seasons[(team_id, season)] = 0\n",
    "\n",
    "    while len(team_seasons_to_scrape) > 0:\n",
    "        still_failing = []\n",
    "        for team_id, season in team_seasons_to_scrape:\n",
    "            try:\n",
    "                scrape_team_season(team_id, season, all_player_ids)\n",
    "            except Exception as e:\n",
    "                failed_team_seasons[(team_id, season)] += 1\n",
    "                attempts = failed_team_seasons[(team_id, season)]\n",
    "                if attempts >= MAX_RETRIES:\n",
    "                    msg = f\"team {team_id} {season}: {e}\"\n",
    "                    log_row(f\"FAILED team season after {MAX_RETRIES} attempts: {team_id} {season} -- error: {e}\")\n",
    "                    permanently_failed.append(msg)\n",
    "                else:\n",
    "                    log_row(f\"failed team season: {team_id} {season} -- attempt {attempts} -- error: {e}\")\n",
    "                    still_failing.append((team_id, season))\n",
    "        team_seasons_to_scrape = still_failing\n",
    "\n",
    "    print()\n",
    "    print(f\"total unique player IDs collected from team rosters: {len(all_player_ids)}\")\n",
    "    log_row(f\"total unique player IDs collected: {len(all_player_ids)}\")\n",
    "\n",
    "    # nit scrape player pages to get player bios and season stats\n",
    "    print()\n",
    "    print(\"Init scraping player pages \")\n",
    "\n",
    "    player_list = sorted(all_player_ids)\n",
    "    failed_players = {}  # player_id: attempt_count\n",
    "\n",
    "    for pid in player_list:\n",
    "        failed_players[pid] = 0\n",
    "\n",
    "    players_to_scrape = list(player_list)\n",
    "\n",
    "    while len(players_to_scrape) > 0:\n",
    "        still_failing = []\n",
    "        for pid in players_to_scrape:\n",
    "            try:\n",
    "                attempt_num = failed_players[pid] + 1\n",
    "                log_row(f\"scraping player page: {pid} -- attempt {attempt_num}\")\n",
    "                scrape_player_page(pid)\n",
    "            except Exception as e:\n",
    "                failed_players[pid] += 1\n",
    "                attempts = failed_players[pid]\n",
    "                if attempts >= MAX_RETRIES:\n",
    "                    msg = f\"player {pid}: {e}\"\n",
    "                    log_row(f\"FAILED player after {MAX_RETRIES} attempts: {pid} -- error: {e}\")\n",
    "                    permanently_failed.append(msg)\n",
    "                else:\n",
    "                    log_row(f\"failed player: {pid} -- attempt {attempts} -- error: {e}\")\n",
    "                    still_failing.append(pid)\n",
    "        players_to_scrape = still_failing\n",
    "        if len(still_failing) > 0:\n",
    "            log_row(f\"retrying {len(still_failing)} failed players: {still_failing}\")\n",
    "\n",
    "    # Init scrape game results for each season\n",
    "    print()\n",
    "    print(\"Init scraping game results for each season\")\n",
    "\n",
    "    failed_game_seasons = {}  # season: attempt_count\n",
    "    for season in SEASONS:\n",
    "        failed_game_seasons[season] = 0\n",
    "\n",
    "    game_seasons_to_scrape = list(SEASONS)\n",
    "\n",
    "    while len(game_seasons_to_scrape) > 0:\n",
    "        still_failing = []\n",
    "        for season in game_seasons_to_scrape:\n",
    "            try:\n",
    "                scrape_games_for_season(season)\n",
    "            except Exception as e:\n",
    "                failed_game_seasons[season] += 1\n",
    "                attempts = failed_game_seasons[season]\n",
    "                season_label = f\"{season - 1}-{str(season)[2:]}\"\n",
    "                if attempts >= MAX_RETRIES:\n",
    "                    msg = f\"games {season_label}: {e}\"\n",
    "                    log_row(f\"FAILED games for season after {MAX_RETRIES} attempts: {season_label} -- error: {e}\")\n",
    "                    permanently_failed.append(msg)\n",
    "                else:\n",
    "                    log_row(f\"failed games for season {season_label} -- attempt {attempts} -- error: {e}\")\n",
    "                    still_failing.append(season)\n",
    "        game_seasons_to_scrape = still_failing\n",
    "\n",
    "    print()\n",
    "    print(\"SCRAPING COMPLETE\")\n",
    "\n",
    "    if len(permanently_failed) == 0:\n",
    "        log_row(\"everything succeeded, no failures\")\n",
    "    else:\n",
    "        log_row(f\"{len(permanently_failed)} item(s) failed all {MAX_RETRIES} attempts:\")\n",
    "        for item in permanently_failed:\n",
    "            log_row(f\"  * {item}\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9000001",
   "metadata": {},
   "source": [
    "## 9. Validation\n",
    "\n",
    "Run queries to verify the database was populated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kevin Schroeder\n",
    "# Validation queries to verify database was populated correctly.\n",
    "\n",
    "def validate_database():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # row counts\n",
    "    print(\"row counts:\")\n",
    "    tables = [\"players\", \"teams\", \"player_season_stats\", \"team_season_stats\", \"games\"]\n",
    "    for table_name in tables:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"  {table_name}: {count:,}\")\n",
    "\n",
    "    # spot check lebron\n",
    "    print()\n",
    "    print(\"spot check -- LeBron James (jamesle01):\")\n",
    "    cursor.execute(\"SELECT * FROM players WHERE player_id = 'jamesle01'\")\n",
    "    row = cursor.fetchone()\n",
    "    if row:\n",
    "        print(f\"  name: {row[1]}\")\n",
    "        print(f\"  born: {row[2]}\")\n",
    "        print(f\"  height/weight: {row[3]}, {row[4]} lb\")\n",
    "        print(f\"  position: {row[5]}, shoots: {row[6]}\")\n",
    "        print(f\"  draft: {row[10]} / round {row[8]} / pick {row[9]} / {row[7]}\")\n",
    "    else:\n",
    "        print(\"  not found in db\")\n",
    "\n",
    "    cursor.execute(\n",
    "        \"SELECT season, team_id, points, rebounds, assists\"\n",
    "        \"FROM player_season_stats \"\n",
    "        \"WHERE player_id = 'jamesle01' \"\n",
    "        \"ORDER BY season\"\n",
    "    )\n",
    "    lebron_seasons = cursor.fetchall()\n",
    "    print(f\"  seasons on file: {len(lebron_seasons)}\")\n",
    "    for s in lebron_seasons:\n",
    "        print(f\"    {s[0]} ({s[1]}): {s[2]} ppg / {s[3]} rpg / {s[4]} apg\")\n",
    "\n",
    "    # spot check lakers 2020\n",
    "    print()\n",
    "    print(\"spot check -- LAL 2019-20:\")\n",
    "    cursor.execute(\n",
    "        \"SELECT * FROM team_season_stats WHERE team_id = 'LAL' AND season = 2020\"\n",
    "    )\n",
    "    row = cursor.fetchone()\n",
    "    if row:\n",
    "        print(f\"  record: {row[2]}-{row[3]} (win%: {row[4]})\")\n",
    "        print(f\"  pace: {row[5]}, ortg: {row[6]}, drtg: {row[7]}, srs: {row[8]}\")\n",
    "    else:\n",
    "        print(\"  not found in db\")\n",
    "\n",
    "    # games check\n",
    "    print()\n",
    "    print(\"spot check -- games:\")\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM games WHERE season = 2024\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"  2023-24 season: {count} games\")\n",
    "\n",
    "    cursor.execute(\"SELECT * FROM games ORDER BY game_date LIMIT 5\")\n",
    "    sample_games = cursor.fetchall()\n",
    "    print(f\"  first 5 games in db:\")\n",
    "    for game in sample_games:\n",
    "        winner = \"home W\" if game[7] else \"away W\"\n",
    "        print(f\"    {game[2]} -- {game[4]} at {game[3]}, {game[6]}-{game[5]} ({winner})\")\n",
    "\n",
    "    # season coverage\n",
    "    print()\n",
    "    cursor.execute(\"SELECT DISTINCT season FROM team_season_stats ORDER BY season\")\n",
    "    seasons_found = [r[0] for r in cursor.fetchall()]\n",
    "    if seasons_found:\n",
    "        print(f\"seasons in team_season_stats: {seasons_found[0]} to {seasons_found[-1]} ({len(seasons_found)} total)\")\n",
    "    else:\n",
    "        print(\"no seasons found in team_season_stats\")\n",
    "\n",
    "    conn.close()\n",
    "    print()\n",
    "    print(\"done validating\")\n",
    "\n",
    "\n",
    "validate_database()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
